{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Experiment9_J024_Pranav_Kotak.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM6No8+6nBYzNHcqzjxD/0w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranavkotak8/ML/blob/master/ML_Experiment9_J024_Pranav_Kotak.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IAtCsnT7A_z"
      },
      "source": [
        "**Importing the Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDmfLqwQPK7X"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AhUXZ5yZ3F6"
      },
      "source": [
        "**1st Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2RBATe0Q6TO"
      },
      "source": [
        "# 1st Dataset\n",
        "#Connectionist Bench (Sonar, Mines vs. Rocks)\n",
        "path='https://raw.githubusercontent.com/pranavkotak8/Datasets/master/sonar.csv'\n",
        "df1=pd.read_csv(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXKoK4fiSBnl",
        "outputId": "a5c7c414-cfd9-4434-b6d3-c1e27f4dacf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "df1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>attribute_1</th>\n",
              "      <th>attribute_2</th>\n",
              "      <th>attribute_3</th>\n",
              "      <th>attribute_4</th>\n",
              "      <th>attribute_5</th>\n",
              "      <th>attribute_6</th>\n",
              "      <th>attribute_7</th>\n",
              "      <th>attribute_8</th>\n",
              "      <th>attribute_9</th>\n",
              "      <th>attribute_10</th>\n",
              "      <th>attribute_11</th>\n",
              "      <th>attribute_12</th>\n",
              "      <th>attribute_13</th>\n",
              "      <th>attribute_14</th>\n",
              "      <th>attribute_15</th>\n",
              "      <th>attribute_16</th>\n",
              "      <th>attribute_17</th>\n",
              "      <th>attribute_18</th>\n",
              "      <th>attribute_19</th>\n",
              "      <th>attribute_20</th>\n",
              "      <th>attribute_21</th>\n",
              "      <th>attribute_22</th>\n",
              "      <th>attribute_23</th>\n",
              "      <th>attribute_24</th>\n",
              "      <th>attribute_25</th>\n",
              "      <th>attribute_26</th>\n",
              "      <th>attribute_27</th>\n",
              "      <th>attribute_28</th>\n",
              "      <th>attribute_29</th>\n",
              "      <th>attribute_30</th>\n",
              "      <th>attribute_31</th>\n",
              "      <th>attribute_32</th>\n",
              "      <th>attribute_33</th>\n",
              "      <th>attribute_34</th>\n",
              "      <th>attribute_35</th>\n",
              "      <th>attribute_36</th>\n",
              "      <th>attribute_37</th>\n",
              "      <th>attribute_38</th>\n",
              "      <th>attribute_39</th>\n",
              "      <th>attribute_40</th>\n",
              "      <th>attribute_41</th>\n",
              "      <th>attribute_42</th>\n",
              "      <th>attribute_43</th>\n",
              "      <th>attribute_44</th>\n",
              "      <th>attribute_45</th>\n",
              "      <th>attribute_46</th>\n",
              "      <th>attribute_47</th>\n",
              "      <th>attribute_48</th>\n",
              "      <th>attribute_49</th>\n",
              "      <th>attribute_50</th>\n",
              "      <th>attribute_51</th>\n",
              "      <th>attribute_52</th>\n",
              "      <th>attribute_53</th>\n",
              "      <th>attribute_54</th>\n",
              "      <th>attribute_55</th>\n",
              "      <th>attribute_56</th>\n",
              "      <th>attribute_57</th>\n",
              "      <th>attribute_58</th>\n",
              "      <th>attribute_59</th>\n",
              "      <th>attribute_60</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0200</td>\n",
              "      <td>0.0371</td>\n",
              "      <td>0.0428</td>\n",
              "      <td>0.0207</td>\n",
              "      <td>0.0954</td>\n",
              "      <td>0.0986</td>\n",
              "      <td>0.1539</td>\n",
              "      <td>0.1601</td>\n",
              "      <td>0.3109</td>\n",
              "      <td>0.2111</td>\n",
              "      <td>0.1609</td>\n",
              "      <td>0.1582</td>\n",
              "      <td>0.2238</td>\n",
              "      <td>0.0645</td>\n",
              "      <td>0.0660</td>\n",
              "      <td>0.2273</td>\n",
              "      <td>0.3100</td>\n",
              "      <td>0.2999</td>\n",
              "      <td>0.5078</td>\n",
              "      <td>0.4797</td>\n",
              "      <td>0.5783</td>\n",
              "      <td>0.5071</td>\n",
              "      <td>0.4328</td>\n",
              "      <td>0.5550</td>\n",
              "      <td>0.6711</td>\n",
              "      <td>0.6415</td>\n",
              "      <td>0.7104</td>\n",
              "      <td>0.8080</td>\n",
              "      <td>0.6791</td>\n",
              "      <td>0.3857</td>\n",
              "      <td>0.1307</td>\n",
              "      <td>0.2604</td>\n",
              "      <td>0.5121</td>\n",
              "      <td>0.7547</td>\n",
              "      <td>0.8537</td>\n",
              "      <td>0.8507</td>\n",
              "      <td>0.6692</td>\n",
              "      <td>0.6097</td>\n",
              "      <td>0.4943</td>\n",
              "      <td>0.2744</td>\n",
              "      <td>0.0510</td>\n",
              "      <td>0.2834</td>\n",
              "      <td>0.2825</td>\n",
              "      <td>0.4256</td>\n",
              "      <td>0.2641</td>\n",
              "      <td>0.1386</td>\n",
              "      <td>0.1051</td>\n",
              "      <td>0.1343</td>\n",
              "      <td>0.0383</td>\n",
              "      <td>0.0324</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>0.0065</td>\n",
              "      <td>0.0159</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0167</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>Rock</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0453</td>\n",
              "      <td>0.0523</td>\n",
              "      <td>0.0843</td>\n",
              "      <td>0.0689</td>\n",
              "      <td>0.1183</td>\n",
              "      <td>0.2583</td>\n",
              "      <td>0.2156</td>\n",
              "      <td>0.3481</td>\n",
              "      <td>0.3337</td>\n",
              "      <td>0.2872</td>\n",
              "      <td>0.4918</td>\n",
              "      <td>0.6552</td>\n",
              "      <td>0.6919</td>\n",
              "      <td>0.7797</td>\n",
              "      <td>0.7464</td>\n",
              "      <td>0.9444</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.8874</td>\n",
              "      <td>0.8024</td>\n",
              "      <td>0.7818</td>\n",
              "      <td>0.5212</td>\n",
              "      <td>0.4052</td>\n",
              "      <td>0.3957</td>\n",
              "      <td>0.3914</td>\n",
              "      <td>0.3250</td>\n",
              "      <td>0.3200</td>\n",
              "      <td>0.3271</td>\n",
              "      <td>0.2767</td>\n",
              "      <td>0.4423</td>\n",
              "      <td>0.2028</td>\n",
              "      <td>0.3788</td>\n",
              "      <td>0.2947</td>\n",
              "      <td>0.1984</td>\n",
              "      <td>0.2341</td>\n",
              "      <td>0.1306</td>\n",
              "      <td>0.4182</td>\n",
              "      <td>0.3835</td>\n",
              "      <td>0.1057</td>\n",
              "      <td>0.1840</td>\n",
              "      <td>0.1970</td>\n",
              "      <td>0.1674</td>\n",
              "      <td>0.0583</td>\n",
              "      <td>0.1401</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.0621</td>\n",
              "      <td>0.0203</td>\n",
              "      <td>0.0530</td>\n",
              "      <td>0.0742</td>\n",
              "      <td>0.0409</td>\n",
              "      <td>0.0061</td>\n",
              "      <td>0.0125</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0089</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>0.0191</td>\n",
              "      <td>0.0140</td>\n",
              "      <td>0.0049</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>Rock</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0262</td>\n",
              "      <td>0.0582</td>\n",
              "      <td>0.1099</td>\n",
              "      <td>0.1083</td>\n",
              "      <td>0.0974</td>\n",
              "      <td>0.2280</td>\n",
              "      <td>0.2431</td>\n",
              "      <td>0.3771</td>\n",
              "      <td>0.5598</td>\n",
              "      <td>0.6194</td>\n",
              "      <td>0.6333</td>\n",
              "      <td>0.7060</td>\n",
              "      <td>0.5544</td>\n",
              "      <td>0.5320</td>\n",
              "      <td>0.6479</td>\n",
              "      <td>0.6931</td>\n",
              "      <td>0.6759</td>\n",
              "      <td>0.7551</td>\n",
              "      <td>0.8929</td>\n",
              "      <td>0.8619</td>\n",
              "      <td>0.7974</td>\n",
              "      <td>0.6737</td>\n",
              "      <td>0.4293</td>\n",
              "      <td>0.3648</td>\n",
              "      <td>0.5331</td>\n",
              "      <td>0.2413</td>\n",
              "      <td>0.5070</td>\n",
              "      <td>0.8533</td>\n",
              "      <td>0.6036</td>\n",
              "      <td>0.8514</td>\n",
              "      <td>0.8512</td>\n",
              "      <td>0.5045</td>\n",
              "      <td>0.1862</td>\n",
              "      <td>0.2709</td>\n",
              "      <td>0.4232</td>\n",
              "      <td>0.3043</td>\n",
              "      <td>0.6116</td>\n",
              "      <td>0.6756</td>\n",
              "      <td>0.5375</td>\n",
              "      <td>0.4719</td>\n",
              "      <td>0.4647</td>\n",
              "      <td>0.2587</td>\n",
              "      <td>0.2129</td>\n",
              "      <td>0.2222</td>\n",
              "      <td>0.2111</td>\n",
              "      <td>0.0176</td>\n",
              "      <td>0.1348</td>\n",
              "      <td>0.0744</td>\n",
              "      <td>0.0130</td>\n",
              "      <td>0.0106</td>\n",
              "      <td>0.0033</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0166</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0244</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0164</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0078</td>\n",
              "      <td>Rock</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0100</td>\n",
              "      <td>0.0171</td>\n",
              "      <td>0.0623</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0368</td>\n",
              "      <td>0.1098</td>\n",
              "      <td>0.1276</td>\n",
              "      <td>0.0598</td>\n",
              "      <td>0.1264</td>\n",
              "      <td>0.0881</td>\n",
              "      <td>0.1992</td>\n",
              "      <td>0.0184</td>\n",
              "      <td>0.2261</td>\n",
              "      <td>0.1729</td>\n",
              "      <td>0.2131</td>\n",
              "      <td>0.0693</td>\n",
              "      <td>0.2281</td>\n",
              "      <td>0.4060</td>\n",
              "      <td>0.3973</td>\n",
              "      <td>0.2741</td>\n",
              "      <td>0.3690</td>\n",
              "      <td>0.5556</td>\n",
              "      <td>0.4846</td>\n",
              "      <td>0.3140</td>\n",
              "      <td>0.5334</td>\n",
              "      <td>0.5256</td>\n",
              "      <td>0.2520</td>\n",
              "      <td>0.2090</td>\n",
              "      <td>0.3559</td>\n",
              "      <td>0.6260</td>\n",
              "      <td>0.7340</td>\n",
              "      <td>0.6120</td>\n",
              "      <td>0.3497</td>\n",
              "      <td>0.3953</td>\n",
              "      <td>0.3012</td>\n",
              "      <td>0.5408</td>\n",
              "      <td>0.8814</td>\n",
              "      <td>0.9857</td>\n",
              "      <td>0.9167</td>\n",
              "      <td>0.6121</td>\n",
              "      <td>0.5006</td>\n",
              "      <td>0.3210</td>\n",
              "      <td>0.3202</td>\n",
              "      <td>0.4295</td>\n",
              "      <td>0.3654</td>\n",
              "      <td>0.2655</td>\n",
              "      <td>0.1576</td>\n",
              "      <td>0.0681</td>\n",
              "      <td>0.0294</td>\n",
              "      <td>0.0241</td>\n",
              "      <td>0.0121</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0150</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0073</td>\n",
              "      <td>0.0050</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0117</td>\n",
              "      <td>Rock</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0762</td>\n",
              "      <td>0.0666</td>\n",
              "      <td>0.0481</td>\n",
              "      <td>0.0394</td>\n",
              "      <td>0.0590</td>\n",
              "      <td>0.0649</td>\n",
              "      <td>0.1209</td>\n",
              "      <td>0.2467</td>\n",
              "      <td>0.3564</td>\n",
              "      <td>0.4459</td>\n",
              "      <td>0.4152</td>\n",
              "      <td>0.3952</td>\n",
              "      <td>0.4256</td>\n",
              "      <td>0.4135</td>\n",
              "      <td>0.4528</td>\n",
              "      <td>0.5326</td>\n",
              "      <td>0.7306</td>\n",
              "      <td>0.6193</td>\n",
              "      <td>0.2032</td>\n",
              "      <td>0.4636</td>\n",
              "      <td>0.4148</td>\n",
              "      <td>0.4292</td>\n",
              "      <td>0.5730</td>\n",
              "      <td>0.5399</td>\n",
              "      <td>0.3161</td>\n",
              "      <td>0.2285</td>\n",
              "      <td>0.6995</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.7262</td>\n",
              "      <td>0.4724</td>\n",
              "      <td>0.5103</td>\n",
              "      <td>0.5459</td>\n",
              "      <td>0.2881</td>\n",
              "      <td>0.0981</td>\n",
              "      <td>0.1951</td>\n",
              "      <td>0.4181</td>\n",
              "      <td>0.4604</td>\n",
              "      <td>0.3217</td>\n",
              "      <td>0.2828</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.1979</td>\n",
              "      <td>0.2444</td>\n",
              "      <td>0.1847</td>\n",
              "      <td>0.0841</td>\n",
              "      <td>0.0692</td>\n",
              "      <td>0.0528</td>\n",
              "      <td>0.0357</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0230</td>\n",
              "      <td>0.0046</td>\n",
              "      <td>0.0156</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>0.0054</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0110</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0107</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>Rock</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>0.0187</td>\n",
              "      <td>0.0346</td>\n",
              "      <td>0.0168</td>\n",
              "      <td>0.0177</td>\n",
              "      <td>0.0393</td>\n",
              "      <td>0.1630</td>\n",
              "      <td>0.2028</td>\n",
              "      <td>0.1694</td>\n",
              "      <td>0.2328</td>\n",
              "      <td>0.2684</td>\n",
              "      <td>0.3108</td>\n",
              "      <td>0.2933</td>\n",
              "      <td>0.2275</td>\n",
              "      <td>0.0994</td>\n",
              "      <td>0.1801</td>\n",
              "      <td>0.2200</td>\n",
              "      <td>0.2732</td>\n",
              "      <td>0.2862</td>\n",
              "      <td>0.2034</td>\n",
              "      <td>0.1740</td>\n",
              "      <td>0.4130</td>\n",
              "      <td>0.6879</td>\n",
              "      <td>0.8120</td>\n",
              "      <td>0.8453</td>\n",
              "      <td>0.8919</td>\n",
              "      <td>0.9300</td>\n",
              "      <td>0.9987</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.8104</td>\n",
              "      <td>0.6199</td>\n",
              "      <td>0.6041</td>\n",
              "      <td>0.5547</td>\n",
              "      <td>0.4160</td>\n",
              "      <td>0.1472</td>\n",
              "      <td>0.0849</td>\n",
              "      <td>0.0608</td>\n",
              "      <td>0.0969</td>\n",
              "      <td>0.1411</td>\n",
              "      <td>0.1676</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.1201</td>\n",
              "      <td>0.1036</td>\n",
              "      <td>0.1977</td>\n",
              "      <td>0.1339</td>\n",
              "      <td>0.0902</td>\n",
              "      <td>0.1085</td>\n",
              "      <td>0.1521</td>\n",
              "      <td>0.1363</td>\n",
              "      <td>0.0858</td>\n",
              "      <td>0.0290</td>\n",
              "      <td>0.0203</td>\n",
              "      <td>0.0116</td>\n",
              "      <td>0.0098</td>\n",
              "      <td>0.0199</td>\n",
              "      <td>0.0033</td>\n",
              "      <td>0.0101</td>\n",
              "      <td>0.0065</td>\n",
              "      <td>0.0115</td>\n",
              "      <td>0.0193</td>\n",
              "      <td>0.0157</td>\n",
              "      <td>Mine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>0.0323</td>\n",
              "      <td>0.0101</td>\n",
              "      <td>0.0298</td>\n",
              "      <td>0.0564</td>\n",
              "      <td>0.0760</td>\n",
              "      <td>0.0958</td>\n",
              "      <td>0.0990</td>\n",
              "      <td>0.1018</td>\n",
              "      <td>0.1030</td>\n",
              "      <td>0.2154</td>\n",
              "      <td>0.3085</td>\n",
              "      <td>0.3425</td>\n",
              "      <td>0.2990</td>\n",
              "      <td>0.1402</td>\n",
              "      <td>0.1235</td>\n",
              "      <td>0.1534</td>\n",
              "      <td>0.1901</td>\n",
              "      <td>0.2429</td>\n",
              "      <td>0.2120</td>\n",
              "      <td>0.2395</td>\n",
              "      <td>0.3272</td>\n",
              "      <td>0.5949</td>\n",
              "      <td>0.8302</td>\n",
              "      <td>0.9045</td>\n",
              "      <td>0.9888</td>\n",
              "      <td>0.9912</td>\n",
              "      <td>0.9448</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9092</td>\n",
              "      <td>0.7412</td>\n",
              "      <td>0.7691</td>\n",
              "      <td>0.7117</td>\n",
              "      <td>0.5304</td>\n",
              "      <td>0.2131</td>\n",
              "      <td>0.0928</td>\n",
              "      <td>0.1297</td>\n",
              "      <td>0.1159</td>\n",
              "      <td>0.1226</td>\n",
              "      <td>0.1768</td>\n",
              "      <td>0.0345</td>\n",
              "      <td>0.1562</td>\n",
              "      <td>0.0824</td>\n",
              "      <td>0.1149</td>\n",
              "      <td>0.1694</td>\n",
              "      <td>0.0954</td>\n",
              "      <td>0.0080</td>\n",
              "      <td>0.0790</td>\n",
              "      <td>0.1255</td>\n",
              "      <td>0.0647</td>\n",
              "      <td>0.0179</td>\n",
              "      <td>0.0051</td>\n",
              "      <td>0.0061</td>\n",
              "      <td>0.0093</td>\n",
              "      <td>0.0135</td>\n",
              "      <td>0.0063</td>\n",
              "      <td>0.0063</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0.0062</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>Mine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205</th>\n",
              "      <td>0.0522</td>\n",
              "      <td>0.0437</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0292</td>\n",
              "      <td>0.0351</td>\n",
              "      <td>0.1171</td>\n",
              "      <td>0.1257</td>\n",
              "      <td>0.1178</td>\n",
              "      <td>0.1258</td>\n",
              "      <td>0.2529</td>\n",
              "      <td>0.2716</td>\n",
              "      <td>0.2374</td>\n",
              "      <td>0.1878</td>\n",
              "      <td>0.0983</td>\n",
              "      <td>0.0683</td>\n",
              "      <td>0.1503</td>\n",
              "      <td>0.1723</td>\n",
              "      <td>0.2339</td>\n",
              "      <td>0.1962</td>\n",
              "      <td>0.1395</td>\n",
              "      <td>0.3164</td>\n",
              "      <td>0.5888</td>\n",
              "      <td>0.7631</td>\n",
              "      <td>0.8473</td>\n",
              "      <td>0.9424</td>\n",
              "      <td>0.9986</td>\n",
              "      <td>0.9699</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.8630</td>\n",
              "      <td>0.6979</td>\n",
              "      <td>0.7717</td>\n",
              "      <td>0.7305</td>\n",
              "      <td>0.5197</td>\n",
              "      <td>0.1786</td>\n",
              "      <td>0.1098</td>\n",
              "      <td>0.1446</td>\n",
              "      <td>0.1066</td>\n",
              "      <td>0.1440</td>\n",
              "      <td>0.1929</td>\n",
              "      <td>0.0325</td>\n",
              "      <td>0.1490</td>\n",
              "      <td>0.0328</td>\n",
              "      <td>0.0537</td>\n",
              "      <td>0.1309</td>\n",
              "      <td>0.0910</td>\n",
              "      <td>0.0757</td>\n",
              "      <td>0.1059</td>\n",
              "      <td>0.1005</td>\n",
              "      <td>0.0535</td>\n",
              "      <td>0.0235</td>\n",
              "      <td>0.0155</td>\n",
              "      <td>0.0160</td>\n",
              "      <td>0.0029</td>\n",
              "      <td>0.0051</td>\n",
              "      <td>0.0062</td>\n",
              "      <td>0.0089</td>\n",
              "      <td>0.0140</td>\n",
              "      <td>0.0138</td>\n",
              "      <td>0.0077</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>Mine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>0.0303</td>\n",
              "      <td>0.0353</td>\n",
              "      <td>0.0490</td>\n",
              "      <td>0.0608</td>\n",
              "      <td>0.0167</td>\n",
              "      <td>0.1354</td>\n",
              "      <td>0.1465</td>\n",
              "      <td>0.1123</td>\n",
              "      <td>0.1945</td>\n",
              "      <td>0.2354</td>\n",
              "      <td>0.2898</td>\n",
              "      <td>0.2812</td>\n",
              "      <td>0.1578</td>\n",
              "      <td>0.0273</td>\n",
              "      <td>0.0673</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.2070</td>\n",
              "      <td>0.2645</td>\n",
              "      <td>0.2828</td>\n",
              "      <td>0.4293</td>\n",
              "      <td>0.5685</td>\n",
              "      <td>0.6990</td>\n",
              "      <td>0.7246</td>\n",
              "      <td>0.7622</td>\n",
              "      <td>0.9242</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9979</td>\n",
              "      <td>0.8297</td>\n",
              "      <td>0.7032</td>\n",
              "      <td>0.7141</td>\n",
              "      <td>0.6893</td>\n",
              "      <td>0.4961</td>\n",
              "      <td>0.2584</td>\n",
              "      <td>0.0969</td>\n",
              "      <td>0.0776</td>\n",
              "      <td>0.0364</td>\n",
              "      <td>0.1572</td>\n",
              "      <td>0.1823</td>\n",
              "      <td>0.1349</td>\n",
              "      <td>0.0849</td>\n",
              "      <td>0.0492</td>\n",
              "      <td>0.1367</td>\n",
              "      <td>0.1552</td>\n",
              "      <td>0.1548</td>\n",
              "      <td>0.1319</td>\n",
              "      <td>0.0985</td>\n",
              "      <td>0.1258</td>\n",
              "      <td>0.0954</td>\n",
              "      <td>0.0489</td>\n",
              "      <td>0.0241</td>\n",
              "      <td>0.0042</td>\n",
              "      <td>0.0086</td>\n",
              "      <td>0.0046</td>\n",
              "      <td>0.0126</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0035</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>0.0079</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>Mine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207</th>\n",
              "      <td>0.0260</td>\n",
              "      <td>0.0363</td>\n",
              "      <td>0.0136</td>\n",
              "      <td>0.0272</td>\n",
              "      <td>0.0214</td>\n",
              "      <td>0.0338</td>\n",
              "      <td>0.0655</td>\n",
              "      <td>0.1400</td>\n",
              "      <td>0.1843</td>\n",
              "      <td>0.2354</td>\n",
              "      <td>0.2720</td>\n",
              "      <td>0.2442</td>\n",
              "      <td>0.1665</td>\n",
              "      <td>0.0336</td>\n",
              "      <td>0.1302</td>\n",
              "      <td>0.1708</td>\n",
              "      <td>0.2177</td>\n",
              "      <td>0.3175</td>\n",
              "      <td>0.3714</td>\n",
              "      <td>0.4552</td>\n",
              "      <td>0.5700</td>\n",
              "      <td>0.7397</td>\n",
              "      <td>0.8062</td>\n",
              "      <td>0.8837</td>\n",
              "      <td>0.9432</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9375</td>\n",
              "      <td>0.7603</td>\n",
              "      <td>0.7123</td>\n",
              "      <td>0.8358</td>\n",
              "      <td>0.7622</td>\n",
              "      <td>0.4567</td>\n",
              "      <td>0.1715</td>\n",
              "      <td>0.1549</td>\n",
              "      <td>0.1641</td>\n",
              "      <td>0.1869</td>\n",
              "      <td>0.2655</td>\n",
              "      <td>0.1713</td>\n",
              "      <td>0.0959</td>\n",
              "      <td>0.0768</td>\n",
              "      <td>0.0847</td>\n",
              "      <td>0.2076</td>\n",
              "      <td>0.2505</td>\n",
              "      <td>0.1862</td>\n",
              "      <td>0.1439</td>\n",
              "      <td>0.1470</td>\n",
              "      <td>0.0991</td>\n",
              "      <td>0.0041</td>\n",
              "      <td>0.0154</td>\n",
              "      <td>0.0116</td>\n",
              "      <td>0.0181</td>\n",
              "      <td>0.0146</td>\n",
              "      <td>0.0129</td>\n",
              "      <td>0.0047</td>\n",
              "      <td>0.0039</td>\n",
              "      <td>0.0061</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0061</td>\n",
              "      <td>0.0115</td>\n",
              "      <td>Mine</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>208 rows Ã— 61 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     attribute_1  attribute_2  attribute_3  ...  attribute_59  attribute_60  Class\n",
              "0         0.0200       0.0371       0.0428  ...        0.0090        0.0032   Rock\n",
              "1         0.0453       0.0523       0.0843  ...        0.0052        0.0044   Rock\n",
              "2         0.0262       0.0582       0.1099  ...        0.0095        0.0078   Rock\n",
              "3         0.0100       0.0171       0.0623  ...        0.0040        0.0117   Rock\n",
              "4         0.0762       0.0666       0.0481  ...        0.0107        0.0094   Rock\n",
              "..           ...          ...          ...  ...           ...           ...    ...\n",
              "203       0.0187       0.0346       0.0168  ...        0.0193        0.0157   Mine\n",
              "204       0.0323       0.0101       0.0298  ...        0.0062        0.0067   Mine\n",
              "205       0.0522       0.0437       0.0180  ...        0.0077        0.0031   Mine\n",
              "206       0.0303       0.0353       0.0490  ...        0.0036        0.0048   Mine\n",
              "207       0.0260       0.0363       0.0136  ...        0.0061        0.0115   Mine\n",
              "\n",
              "[208 rows x 61 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfjlviX1bbBa",
        "outputId": "dd424d4f-195d-47bc-9fe5-edfd341cca6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "# Finding the Missing Values\n",
        "df1.isna().sum()/len(df1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "attribute_1     0.0\n",
              "attribute_2     0.0\n",
              "attribute_3     0.0\n",
              "attribute_4     0.0\n",
              "attribute_5     0.0\n",
              "               ... \n",
              "attribute_57    0.0\n",
              "attribute_58    0.0\n",
              "attribute_59    0.0\n",
              "attribute_60    0.0\n",
              "Class           0.0\n",
              "Length: 61, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxHeR6FMdZKg",
        "outputId": "fbba992d-c1a7-4fd6-ea9a-42c297222784",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df1.info()  # No null values found"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 208 entries, 0 to 207\n",
            "Data columns (total 61 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   attribute_1   208 non-null    float64\n",
            " 1   attribute_2   208 non-null    float64\n",
            " 2   attribute_3   208 non-null    float64\n",
            " 3   attribute_4   208 non-null    float64\n",
            " 4   attribute_5   208 non-null    float64\n",
            " 5   attribute_6   208 non-null    float64\n",
            " 6   attribute_7   208 non-null    float64\n",
            " 7   attribute_8   208 non-null    float64\n",
            " 8   attribute_9   208 non-null    float64\n",
            " 9   attribute_10  208 non-null    float64\n",
            " 10  attribute_11  208 non-null    float64\n",
            " 11  attribute_12  208 non-null    float64\n",
            " 12  attribute_13  208 non-null    float64\n",
            " 13  attribute_14  208 non-null    float64\n",
            " 14  attribute_15  208 non-null    float64\n",
            " 15  attribute_16  208 non-null    float64\n",
            " 16  attribute_17  208 non-null    float64\n",
            " 17  attribute_18  208 non-null    float64\n",
            " 18  attribute_19  208 non-null    float64\n",
            " 19  attribute_20  208 non-null    float64\n",
            " 20  attribute_21  208 non-null    float64\n",
            " 21  attribute_22  208 non-null    float64\n",
            " 22  attribute_23  208 non-null    float64\n",
            " 23  attribute_24  208 non-null    float64\n",
            " 24  attribute_25  208 non-null    float64\n",
            " 25  attribute_26  208 non-null    float64\n",
            " 26  attribute_27  208 non-null    float64\n",
            " 27  attribute_28  208 non-null    float64\n",
            " 28  attribute_29  208 non-null    float64\n",
            " 29  attribute_30  208 non-null    float64\n",
            " 30  attribute_31  208 non-null    float64\n",
            " 31  attribute_32  208 non-null    float64\n",
            " 32  attribute_33  208 non-null    float64\n",
            " 33  attribute_34  208 non-null    float64\n",
            " 34  attribute_35  208 non-null    float64\n",
            " 35  attribute_36  208 non-null    float64\n",
            " 36  attribute_37  208 non-null    float64\n",
            " 37  attribute_38  208 non-null    float64\n",
            " 38  attribute_39  208 non-null    float64\n",
            " 39  attribute_40  208 non-null    float64\n",
            " 40  attribute_41  208 non-null    float64\n",
            " 41  attribute_42  208 non-null    float64\n",
            " 42  attribute_43  208 non-null    float64\n",
            " 43  attribute_44  208 non-null    float64\n",
            " 44  attribute_45  208 non-null    float64\n",
            " 45  attribute_46  208 non-null    float64\n",
            " 46  attribute_47  208 non-null    float64\n",
            " 47  attribute_48  208 non-null    float64\n",
            " 48  attribute_49  208 non-null    float64\n",
            " 49  attribute_50  208 non-null    float64\n",
            " 50  attribute_51  208 non-null    float64\n",
            " 51  attribute_52  208 non-null    float64\n",
            " 52  attribute_53  208 non-null    float64\n",
            " 53  attribute_54  208 non-null    float64\n",
            " 54  attribute_55  208 non-null    float64\n",
            " 55  attribute_56  208 non-null    float64\n",
            " 56  attribute_57  208 non-null    float64\n",
            " 57  attribute_58  208 non-null    float64\n",
            " 58  attribute_59  208 non-null    float64\n",
            " 59  attribute_60  208 non-null    float64\n",
            " 60  Class         208 non-null    object \n",
            "dtypes: float64(60), object(1)\n",
            "memory usage: 99.2+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-llMKx7desV",
        "outputId": "5885069f-c8c4-498f-86b6-1b9d36bc9b86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "# Lets check out the Target Column\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "sns.countplot(df1['Class'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBTrb8XGdu4H"
      },
      "source": [
        "# Normalizing the data\n",
        "target=df1['Class']\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "ln=LabelEncoder()\n",
        "target1=(ln.fit_transform(target))\n",
        "df1.drop(columns={'Class'},inplace=True)\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "m=MinMaxScaler()\n",
        "df1=pd.DataFrame(m.fit_transform(df1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmSZiO9KijhM",
        "outputId": "56ff42df-c00d-4eaa-a5e8-ff393faf260a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "df1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.136431</td>\n",
              "      <td>0.156451</td>\n",
              "      <td>0.135677</td>\n",
              "      <td>0.035426</td>\n",
              "      <td>0.224956</td>\n",
              "      <td>0.237571</td>\n",
              "      <td>0.407468</td>\n",
              "      <td>0.340904</td>\n",
              "      <td>0.449282</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.187154</td>\n",
              "      <td>0.197245</td>\n",
              "      <td>0.295667</td>\n",
              "      <td>0.038362</td>\n",
              "      <td>0.063096</td>\n",
              "      <td>0.214838</td>\n",
              "      <td>0.285048</td>\n",
              "      <td>0.272623</td>\n",
              "      <td>0.482222</td>\n",
              "      <td>0.443172</td>\n",
              "      <td>0.555544</td>\n",
              "      <td>0.496064</td>\n",
              "      <td>0.398962</td>\n",
              "      <td>0.544104</td>\n",
              "      <td>0.663012</td>\n",
              "      <td>0.605133</td>\n",
              "      <td>0.695766</td>\n",
              "      <td>0.802388</td>\n",
              "      <td>0.674412</td>\n",
              "      <td>0.345584</td>\n",
              "      <td>0.089918</td>\n",
              "      <td>0.247135</td>\n",
              "      <td>0.487661</td>\n",
              "      <td>0.777424</td>\n",
              "      <td>0.850363</td>\n",
              "      <td>0.849496</td>\n",
              "      <td>0.693309</td>\n",
              "      <td>0.594156</td>\n",
              "      <td>0.481973</td>\n",
              "      <td>0.286166</td>\n",
              "      <td>0.017371</td>\n",
              "      <td>0.339194</td>\n",
              "      <td>0.365317</td>\n",
              "      <td>0.548312</td>\n",
              "      <td>0.375462</td>\n",
              "      <td>0.190071</td>\n",
              "      <td>0.190330</td>\n",
              "      <td>0.402216</td>\n",
              "      <td>0.193337</td>\n",
              "      <td>0.392727</td>\n",
              "      <td>0.231076</td>\n",
              "      <td>0.027104</td>\n",
              "      <td>0.155844</td>\n",
              "      <td>0.435673</td>\n",
              "      <td>0.149660</td>\n",
              "      <td>0.417949</td>\n",
              "      <td>0.502841</td>\n",
              "      <td>0.185355</td>\n",
              "      <td>0.245179</td>\n",
              "      <td>0.060046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.323009</td>\n",
              "      <td>0.221603</td>\n",
              "      <td>0.272011</td>\n",
              "      <td>0.150024</td>\n",
              "      <td>0.283033</td>\n",
              "      <td>0.666756</td>\n",
              "      <td>0.574405</td>\n",
              "      <td>0.755458</td>\n",
              "      <td>0.483045</td>\n",
              "      <td>0.394537</td>\n",
              "      <td>0.656316</td>\n",
              "      <td>0.925557</td>\n",
              "      <td>0.969483</td>\n",
              "      <td>0.775910</td>\n",
              "      <td>0.745611</td>\n",
              "      <td>0.944637</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.883013</td>\n",
              "      <td>0.792131</td>\n",
              "      <td>0.766481</td>\n",
              "      <td>0.495363</td>\n",
              "      <td>0.391882</td>\n",
              "      <td>0.359648</td>\n",
              "      <td>0.376498</td>\n",
              "      <td>0.308402</td>\n",
              "      <td>0.251019</td>\n",
              "      <td>0.293098</td>\n",
              "      <td>0.255558</td>\n",
              "      <td>0.434152</td>\n",
              "      <td>0.150740</td>\n",
              "      <td>0.360327</td>\n",
              "      <td>0.285666</td>\n",
              "      <td>0.158248</td>\n",
              "      <td>0.225649</td>\n",
              "      <td>0.110770</td>\n",
              "      <td>0.413508</td>\n",
              "      <td>0.380932</td>\n",
              "      <td>0.070084</td>\n",
              "      <td>0.154860</td>\n",
              "      <td>0.201852</td>\n",
              "      <td>0.152171</td>\n",
              "      <td>0.064347</td>\n",
              "      <td>0.181172</td>\n",
              "      <td>0.209740</td>\n",
              "      <td>0.088285</td>\n",
              "      <td>0.027839</td>\n",
              "      <td>0.095980</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.206461</td>\n",
              "      <td>0.073939</td>\n",
              "      <td>0.124502</td>\n",
              "      <td>0.108417</td>\n",
              "      <td>0.218182</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.199546</td>\n",
              "      <td>0.479487</td>\n",
              "      <td>0.389205</td>\n",
              "      <td>0.105263</td>\n",
              "      <td>0.140496</td>\n",
              "      <td>0.087760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.182153</td>\n",
              "      <td>0.246892</td>\n",
              "      <td>0.356110</td>\n",
              "      <td>0.243699</td>\n",
              "      <td>0.230028</td>\n",
              "      <td>0.585327</td>\n",
              "      <td>0.648810</td>\n",
              "      <td>0.819405</td>\n",
              "      <td>0.817859</td>\n",
              "      <td>0.869584</td>\n",
              "      <td>0.856940</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.771556</td>\n",
              "      <td>0.520470</td>\n",
              "      <td>0.646805</td>\n",
              "      <td>0.688887</td>\n",
              "      <td>0.664180</td>\n",
              "      <td>0.745558</td>\n",
              "      <td>0.887334</td>\n",
              "      <td>0.852205</td>\n",
              "      <td>0.786467</td>\n",
              "      <td>0.666394</td>\n",
              "      <td>0.395253</td>\n",
              "      <td>0.349247</td>\n",
              "      <td>0.521619</td>\n",
              "      <td>0.164335</td>\n",
              "      <td>0.482088</td>\n",
              "      <td>0.849012</td>\n",
              "      <td>0.597808</td>\n",
              "      <td>0.841696</td>\n",
              "      <td>0.875204</td>\n",
              "      <td>0.521344</td>\n",
              "      <td>0.145437</td>\n",
              "      <td>0.264653</td>\n",
              "      <td>0.410044</td>\n",
              "      <td>0.298690</td>\n",
              "      <td>0.630330</td>\n",
              "      <td>0.662681</td>\n",
              "      <td>0.527514</td>\n",
              "      <td>0.501307</td>\n",
              "      <td>0.496468</td>\n",
              "      <td>0.309035</td>\n",
              "      <td>0.275314</td>\n",
              "      <td>0.286266</td>\n",
              "      <td>0.300114</td>\n",
              "      <td>0.024136</td>\n",
              "      <td>0.244114</td>\n",
              "      <td>0.222821</td>\n",
              "      <td>0.065623</td>\n",
              "      <td>0.128485</td>\n",
              "      <td>0.032869</td>\n",
              "      <td>0.319544</td>\n",
              "      <td>0.418182</td>\n",
              "      <td>0.248538</td>\n",
              "      <td>0.394558</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.889205</td>\n",
              "      <td>0.368421</td>\n",
              "      <td>0.258953</td>\n",
              "      <td>0.166282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.062684</td>\n",
              "      <td>0.070724</td>\n",
              "      <td>0.199737</td>\n",
              "      <td>0.034950</td>\n",
              "      <td>0.034999</td>\n",
              "      <td>0.071486</td>\n",
              "      <td>0.288149</td>\n",
              "      <td>0.269239</td>\n",
              "      <td>0.077447</td>\n",
              "      <td>0.164593</td>\n",
              "      <td>0.083936</td>\n",
              "      <td>0.257327</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.205012</td>\n",
              "      <td>0.170328</td>\n",
              "      <td>0.200387</td>\n",
              "      <td>0.035644</td>\n",
              "      <td>0.198026</td>\n",
              "      <td>0.375131</td>\n",
              "      <td>0.354987</td>\n",
              "      <td>0.234928</td>\n",
              "      <td>0.354872</td>\n",
              "      <td>0.529088</td>\n",
              "      <td>0.471980</td>\n",
              "      <td>0.297131</td>\n",
              "      <td>0.486067</td>\n",
              "      <td>0.501628</td>\n",
              "      <td>0.230136</td>\n",
              "      <td>0.197443</td>\n",
              "      <td>0.313838</td>\n",
              "      <td>0.629755</td>\n",
              "      <td>0.779151</td>\n",
              "      <td>0.592565</td>\n",
              "      <td>0.348172</td>\n",
              "      <td>0.381508</td>\n",
              "      <td>0.295565</td>\n",
              "      <td>0.552919</td>\n",
              "      <td>0.876677</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.985839</td>\n",
              "      <td>0.667169</td>\n",
              "      <td>0.604396</td>\n",
              "      <td>0.415104</td>\n",
              "      <td>0.412523</td>\n",
              "      <td>0.610606</td>\n",
              "      <td>0.501097</td>\n",
              "      <td>0.480804</td>\n",
              "      <td>0.471998</td>\n",
              "      <td>0.343766</td>\n",
              "      <td>0.356364</td>\n",
              "      <td>0.240040</td>\n",
              "      <td>0.161198</td>\n",
              "      <td>0.080519</td>\n",
              "      <td>0.409357</td>\n",
              "      <td>0.179138</td>\n",
              "      <td>0.176923</td>\n",
              "      <td>0.133523</td>\n",
              "      <td>0.093822</td>\n",
              "      <td>0.107438</td>\n",
              "      <td>0.256351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.550885</td>\n",
              "      <td>0.282898</td>\n",
              "      <td>0.153088</td>\n",
              "      <td>0.079886</td>\n",
              "      <td>0.132640</td>\n",
              "      <td>0.147003</td>\n",
              "      <td>0.318182</td>\n",
              "      <td>0.531863</td>\n",
              "      <td>0.516659</td>\n",
              "      <td>0.621479</td>\n",
              "      <td>0.547710</td>\n",
              "      <td>0.544549</td>\n",
              "      <td>0.586152</td>\n",
              "      <td>0.398268</td>\n",
              "      <td>0.451098</td>\n",
              "      <td>0.525544</td>\n",
              "      <td>0.720858</td>\n",
              "      <td>0.604468</td>\n",
              "      <td>0.161793</td>\n",
              "      <td>0.425942</td>\n",
              "      <td>0.383221</td>\n",
              "      <td>0.416420</td>\n",
              "      <td>0.547526</td>\n",
              "      <td>0.528634</td>\n",
              "      <td>0.299283</td>\n",
              "      <td>0.150237</td>\n",
              "      <td>0.684316</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.722200</td>\n",
              "      <td>0.437946</td>\n",
              "      <td>0.503651</td>\n",
              "      <td>0.567850</td>\n",
              "      <td>0.252441</td>\n",
              "      <td>0.081505</td>\n",
              "      <td>0.176741</td>\n",
              "      <td>0.413407</td>\n",
              "      <td>0.465012</td>\n",
              "      <td>0.294686</td>\n",
              "      <td>0.259013</td>\n",
              "      <td>0.251961</td>\n",
              "      <td>0.187493</td>\n",
              "      <td>0.291575</td>\n",
              "      <td>0.238847</td>\n",
              "      <td>0.108348</td>\n",
              "      <td>0.098379</td>\n",
              "      <td>0.072408</td>\n",
              "      <td>0.064650</td>\n",
              "      <td>0.025457</td>\n",
              "      <td>0.116103</td>\n",
              "      <td>0.055758</td>\n",
              "      <td>0.155378</td>\n",
              "      <td>0.032810</td>\n",
              "      <td>0.127273</td>\n",
              "      <td>0.277778</td>\n",
              "      <td>0.235828</td>\n",
              "      <td>0.028205</td>\n",
              "      <td>0.196023</td>\n",
              "      <td>0.102975</td>\n",
              "      <td>0.292011</td>\n",
              "      <td>0.203233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>0.126844</td>\n",
              "      <td>0.145735</td>\n",
              "      <td>0.050263</td>\n",
              "      <td>0.028293</td>\n",
              "      <td>0.082678</td>\n",
              "      <td>0.410642</td>\n",
              "      <td>0.539773</td>\n",
              "      <td>0.361411</td>\n",
              "      <td>0.333629</td>\n",
              "      <td>0.367653</td>\n",
              "      <td>0.399688</td>\n",
              "      <td>0.395223</td>\n",
              "      <td>0.300993</td>\n",
              "      <td>0.074353</td>\n",
              "      <td>0.177550</td>\n",
              "      <td>0.207409</td>\n",
              "      <td>0.246917</td>\n",
              "      <td>0.258390</td>\n",
              "      <td>0.162003</td>\n",
              "      <td>0.116010</td>\n",
              "      <td>0.381324</td>\n",
              "      <td>0.680912</td>\n",
              "      <td>0.800784</td>\n",
              "      <td>0.841512</td>\n",
              "      <td>0.889242</td>\n",
              "      <td>0.922899</td>\n",
              "      <td>0.998634</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.807630</td>\n",
              "      <td>0.595078</td>\n",
              "      <td>0.605886</td>\n",
              "      <td>0.577735</td>\n",
              "      <td>0.386748</td>\n",
              "      <td>0.133545</td>\n",
              "      <td>0.064028</td>\n",
              "      <td>0.053226</td>\n",
              "      <td>0.067571</td>\n",
              "      <td>0.106894</td>\n",
              "      <td>0.137571</td>\n",
              "      <td>0.117974</td>\n",
              "      <td>0.097394</td>\n",
              "      <td>0.119658</td>\n",
              "      <td>0.255658</td>\n",
              "      <td>0.172507</td>\n",
              "      <td>0.128234</td>\n",
              "      <td>0.148793</td>\n",
              "      <td>0.275444</td>\n",
              "      <td>0.408206</td>\n",
              "      <td>0.433115</td>\n",
              "      <td>0.351515</td>\n",
              "      <td>0.202191</td>\n",
              "      <td>0.154066</td>\n",
              "      <td>0.241558</td>\n",
              "      <td>0.552632</td>\n",
              "      <td>0.061224</td>\n",
              "      <td>0.248718</td>\n",
              "      <td>0.176136</td>\n",
              "      <td>0.256293</td>\n",
              "      <td>0.528926</td>\n",
              "      <td>0.348730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>0.227139</td>\n",
              "      <td>0.040720</td>\n",
              "      <td>0.092970</td>\n",
              "      <td>0.120304</td>\n",
              "      <td>0.175755</td>\n",
              "      <td>0.230046</td>\n",
              "      <td>0.258929</td>\n",
              "      <td>0.212348</td>\n",
              "      <td>0.141419</td>\n",
              "      <td>0.291863</td>\n",
              "      <td>0.396427</td>\n",
              "      <td>0.467321</td>\n",
              "      <td>0.403915</td>\n",
              "      <td>0.116428</td>\n",
              "      <td>0.120774</td>\n",
              "      <td>0.139630</td>\n",
              "      <td>0.160812</td>\n",
              "      <td>0.213403</td>\n",
              "      <td>0.171050</td>\n",
              "      <td>0.186109</td>\n",
              "      <td>0.290894</td>\n",
              "      <td>0.585830</td>\n",
              "      <td>0.820070</td>\n",
              "      <td>0.902162</td>\n",
              "      <td>0.988525</td>\n",
              "      <td>0.990307</td>\n",
              "      <td>0.942011</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.907873</td>\n",
              "      <td>0.724300</td>\n",
              "      <td>0.785722</td>\n",
              "      <td>0.754100</td>\n",
              "      <td>0.506878</td>\n",
              "      <td>0.203392</td>\n",
              "      <td>0.072108</td>\n",
              "      <td>0.122681</td>\n",
              "      <td>0.088345</td>\n",
              "      <td>0.087657</td>\n",
              "      <td>0.147270</td>\n",
              "      <td>0.024837</td>\n",
              "      <td>0.139201</td>\n",
              "      <td>0.093773</td>\n",
              "      <td>0.148584</td>\n",
              "      <td>0.218243</td>\n",
              "      <td>0.135627</td>\n",
              "      <td>0.010971</td>\n",
              "      <td>0.143064</td>\n",
              "      <td>0.375861</td>\n",
              "      <td>0.326603</td>\n",
              "      <td>0.216970</td>\n",
              "      <td>0.050797</td>\n",
              "      <td>0.075606</td>\n",
              "      <td>0.228571</td>\n",
              "      <td>0.365497</td>\n",
              "      <td>0.129252</td>\n",
              "      <td>0.151282</td>\n",
              "      <td>0.088068</td>\n",
              "      <td>0.066362</td>\n",
              "      <td>0.168044</td>\n",
              "      <td>0.140878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205</th>\n",
              "      <td>0.373894</td>\n",
              "      <td>0.184741</td>\n",
              "      <td>0.054205</td>\n",
              "      <td>0.055635</td>\n",
              "      <td>0.072026</td>\n",
              "      <td>0.287288</td>\n",
              "      <td>0.331169</td>\n",
              "      <td>0.247630</td>\n",
              "      <td>0.175181</td>\n",
              "      <td>0.345488</td>\n",
              "      <td>0.344109</td>\n",
              "      <td>0.313306</td>\n",
              "      <td>0.243846</td>\n",
              "      <td>0.073219</td>\n",
              "      <td>0.065403</td>\n",
              "      <td>0.136475</td>\n",
              "      <td>0.142369</td>\n",
              "      <td>0.204052</td>\n",
              "      <td>0.154429</td>\n",
              "      <td>0.079088</td>\n",
              "      <td>0.279511</td>\n",
              "      <td>0.579593</td>\n",
              "      <td>0.748967</td>\n",
              "      <td>0.843561</td>\n",
              "      <td>0.940984</td>\n",
              "      <td>0.998458</td>\n",
              "      <td>0.968379</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.860998</td>\n",
              "      <td>0.678172</td>\n",
              "      <td>0.788556</td>\n",
              "      <td>0.775219</td>\n",
              "      <td>0.495642</td>\n",
              "      <td>0.166826</td>\n",
              "      <td>0.089496</td>\n",
              "      <td>0.137702</td>\n",
              "      <td>0.078176</td>\n",
              "      <td>0.109910</td>\n",
              "      <td>0.164242</td>\n",
              "      <td>0.022658</td>\n",
              "      <td>0.130863</td>\n",
              "      <td>0.033211</td>\n",
              "      <td>0.069443</td>\n",
              "      <td>0.168642</td>\n",
              "      <td>0.129372</td>\n",
              "      <td>0.103812</td>\n",
              "      <td>0.191778</td>\n",
              "      <td>0.300988</td>\n",
              "      <td>0.270066</td>\n",
              "      <td>0.284848</td>\n",
              "      <td>0.154382</td>\n",
              "      <td>0.216833</td>\n",
              "      <td>0.062338</td>\n",
              "      <td>0.119883</td>\n",
              "      <td>0.126984</td>\n",
              "      <td>0.217949</td>\n",
              "      <td>0.389205</td>\n",
              "      <td>0.308924</td>\n",
              "      <td>0.209366</td>\n",
              "      <td>0.057737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>0.212389</td>\n",
              "      <td>0.148736</td>\n",
              "      <td>0.156045</td>\n",
              "      <td>0.130766</td>\n",
              "      <td>0.025361</td>\n",
              "      <td>0.336469</td>\n",
              "      <td>0.387446</td>\n",
              "      <td>0.235502</td>\n",
              "      <td>0.276914</td>\n",
              "      <td>0.320463</td>\n",
              "      <td>0.369914</td>\n",
              "      <td>0.377491</td>\n",
              "      <td>0.200662</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.064400</td>\n",
              "      <td>0.130470</td>\n",
              "      <td>0.178323</td>\n",
              "      <td>0.235844</td>\n",
              "      <td>0.245529</td>\n",
              "      <td>0.389234</td>\n",
              "      <td>0.545215</td>\n",
              "      <td>0.692261</td>\n",
              "      <td>0.708170</td>\n",
              "      <td>0.756377</td>\n",
              "      <td>0.922336</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.997794</td>\n",
              "      <td>0.824722</td>\n",
              "      <td>0.698864</td>\n",
              "      <td>0.695430</td>\n",
              "      <td>0.698747</td>\n",
              "      <td>0.511907</td>\n",
              "      <td>0.221254</td>\n",
              "      <td>0.080233</td>\n",
              "      <td>0.056561</td>\n",
              "      <td>0.028629</td>\n",
              "      <td>0.133501</td>\n",
              "      <td>0.149735</td>\n",
              "      <td>0.103099</td>\n",
              "      <td>0.079739</td>\n",
              "      <td>0.015287</td>\n",
              "      <td>0.160073</td>\n",
              "      <td>0.200698</td>\n",
              "      <td>0.199433</td>\n",
              "      <td>0.187518</td>\n",
              "      <td>0.135080</td>\n",
              "      <td>0.227816</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.246845</td>\n",
              "      <td>0.292121</td>\n",
              "      <td>0.041833</td>\n",
              "      <td>0.111270</td>\n",
              "      <td>0.106494</td>\n",
              "      <td>0.339181</td>\n",
              "      <td>0.068027</td>\n",
              "      <td>0.079487</td>\n",
              "      <td>0.088068</td>\n",
              "      <td>0.173913</td>\n",
              "      <td>0.096419</td>\n",
              "      <td>0.096998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207</th>\n",
              "      <td>0.180678</td>\n",
              "      <td>0.153022</td>\n",
              "      <td>0.039750</td>\n",
              "      <td>0.050880</td>\n",
              "      <td>0.037281</td>\n",
              "      <td>0.063424</td>\n",
              "      <td>0.168290</td>\n",
              "      <td>0.296582</td>\n",
              "      <td>0.261810</td>\n",
              "      <td>0.320463</td>\n",
              "      <td>0.344676</td>\n",
              "      <td>0.323271</td>\n",
              "      <td>0.213186</td>\n",
              "      <td>0.006497</td>\n",
              "      <td>0.127495</td>\n",
              "      <td>0.157338</td>\n",
              "      <td>0.189410</td>\n",
              "      <td>0.290909</td>\n",
              "      <td>0.338733</td>\n",
              "      <td>0.416952</td>\n",
              "      <td>0.546796</td>\n",
              "      <td>0.733872</td>\n",
              "      <td>0.794638</td>\n",
              "      <td>0.880852</td>\n",
              "      <td>0.941803</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.934342</td>\n",
              "      <td>0.753294</td>\n",
              "      <td>0.708097</td>\n",
              "      <td>0.825077</td>\n",
              "      <td>0.778202</td>\n",
              "      <td>0.467648</td>\n",
              "      <td>0.130001</td>\n",
              "      <td>0.141706</td>\n",
              "      <td>0.145034</td>\n",
              "      <td>0.180343</td>\n",
              "      <td>0.251913</td>\n",
              "      <td>0.138297</td>\n",
              "      <td>0.061986</td>\n",
              "      <td>0.070915</td>\n",
              "      <td>0.056398</td>\n",
              "      <td>0.246642</td>\n",
              "      <td>0.323936</td>\n",
              "      <td>0.239887</td>\n",
              "      <td>0.204578</td>\n",
              "      <td>0.201591</td>\n",
              "      <td>0.179464</td>\n",
              "      <td>0.012279</td>\n",
              "      <td>0.077739</td>\n",
              "      <td>0.140606</td>\n",
              "      <td>0.180279</td>\n",
              "      <td>0.196862</td>\n",
              "      <td>0.322078</td>\n",
              "      <td>0.108187</td>\n",
              "      <td>0.074830</td>\n",
              "      <td>0.146154</td>\n",
              "      <td>0.105114</td>\n",
              "      <td>0.075515</td>\n",
              "      <td>0.165289</td>\n",
              "      <td>0.251732</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>208 rows Ã— 60 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0         1         2   ...        57        58        59\n",
              "0    0.136431  0.156451  0.135677  ...  0.185355  0.245179  0.060046\n",
              "1    0.323009  0.221603  0.272011  ...  0.105263  0.140496  0.087760\n",
              "2    0.182153  0.246892  0.356110  ...  0.368421  0.258953  0.166282\n",
              "3    0.062684  0.070724  0.199737  ...  0.093822  0.107438  0.256351\n",
              "4    0.550885  0.282898  0.153088  ...  0.102975  0.292011  0.203233\n",
              "..        ...       ...       ...  ...       ...       ...       ...\n",
              "203  0.126844  0.145735  0.050263  ...  0.256293  0.528926  0.348730\n",
              "204  0.227139  0.040720  0.092970  ...  0.066362  0.168044  0.140878\n",
              "205  0.373894  0.184741  0.054205  ...  0.308924  0.209366  0.057737\n",
              "206  0.212389  0.148736  0.156045  ...  0.173913  0.096419  0.096998\n",
              "207  0.180678  0.153022  0.039750  ...  0.075515  0.165289  0.251732\n",
              "\n",
              "[208 rows x 60 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bo6gAeCiP9xv"
      },
      "source": [
        "from sklearn import linear_model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logistic=linear_model.LogisticRegression()\n",
        "sgd=linear_model.SGDClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nswsaZASQBEe"
      },
      "source": [
        "# Splitting into trainset and testset in the ratio 80:20\n",
        "seed=8\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(df1,target1,test_size=0.2,random_state=seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnZn_nhnQOLM"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "scores=cross_val_score(logistic,X_train,y_train,cv=5,scoring='accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL1bPbaqQsw2",
        "outputId": "91aea34a-ae9e-4f62-a683-5dbaa5f39b66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.mean(scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8130124777183602"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOVMOquCSCOe"
      },
      "source": [
        "scores_sgd=cross_val_score(sgd,X_train,y_train,cv=5,scoring='accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4LYQnSOSEyS",
        "outputId": "5f1ec5d2-8f45-4a4a-c4f7-57bad15aae77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.mean(scores_sgd)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7347593582887699"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1ZAuszfSG7s"
      },
      "source": [
        "# GridSearch CV on Logistic Regression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "gscv_log=GridSearchCV(logistic, {\n",
        "                      'penalty': ['l1','l2','elasticnet'],\n",
        "                      'C': [0.1,0.5,0.25,0.8,0.9,0.95,1.5,2.3,5.5],\n",
        "                      'class_weight' : [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}],\n",
        "                      'solver' : ['liblinear', 'saga']\n",
        "                      },cv=3,return_train_score=False,n_jobs=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBX8F2k4SVWN",
        "outputId": "04ab602b-9c0a-443b-d008-0fc2aae241f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(166,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyqVQCOpSK-M",
        "outputId": "9d92a478-1f49-4e88-ac9b-4f546ac1a17b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "source": [
        "gscv_log.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=3, error_score=nan,\n",
              "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                          fit_intercept=True,\n",
              "                                          intercept_scaling=1, l1_ratio=None,\n",
              "                                          max_iter=100, multi_class='auto',\n",
              "                                          n_jobs=None, penalty='l2',\n",
              "                                          random_state=None, solver='lbfgs',\n",
              "                                          tol=0.0001, verbose=0,\n",
              "                                          warm_start=False),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid={'C': [0.1, 0.5, 0.25, 0.8, 0.9, 0.95, 1.5, 2.3, 5.5],\n",
              "                         'class_weight': [{0: 0.5, 1: 0.5}, {0: 0.6, 1: 0.4}],\n",
              "                         'penalty': ['l1', 'l2', 'elasticnet'],\n",
              "                         'solver': ['liblinear', 'saga']},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrqXuoKRSS9Z",
        "outputId": "4407cd06-5b68-4d41-c128-54b85e8fcf20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "gscv_log.best_score_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8254329004329004"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v26px7JuS2UA",
        "outputId": "914c1ed2-a3e7-4c48-b664-ab016751bb60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "gscv_log.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'C': 5.5, 'class_weight': {0: 0.6, 1: 0.4}, 'penalty': 'l2', 'solver': 'saga'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO7YiRLKS6DI",
        "outputId": "1f237539-477d-496b-b37e-4edc9846fdd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "gscv_log.best_index_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "105"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skz3MVrkTYKC",
        "outputId": "dd8fb9f1-51cc-421c-c635-8741df79e1d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Accuracy on the Test Set\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_test_pred = gscv_log.predict(X_test)\n",
        "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
        "print(\"Accuracy:\"+str(accuracy_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:0.7619047619047619\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwJOaAwxadeD",
        "outputId": "4a576296-6db7-4710-d202-22c4edfaf03c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "# Performing Random Forest on the Dataset\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf=RandomForestClassifier()\n",
        "rf.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ynxk_mVbFI5",
        "outputId": "cbcfef0f-22ed-418e-ee26-614f3e13b847",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_test_pred_rf = rf.predict(X_test)\n",
        "accuracy_test = accuracy_score(y_test, y_test_pred_rf)\n",
        "print(\"Accuracy:\"+str(accuracy_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:0.8333333333333334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fkvt-BJPfBnk"
      },
      "source": [
        "# GridSearch CV on Random Forest\n",
        "gscv_rf= GridSearchCV( RandomForestClassifier(),{\n",
        "                      'n_estimators':[1,10,15,20,52,60,100],\n",
        "                       'criterion':['gini','entropy'],\n",
        "                      'max_depth':[5,10,15,20,25,35,52,85,100],\n",
        "                      'class_weight' : [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}],\n",
        "                      'ccp_alpha':[0.20,0.05,1.5,2.5],\n",
        "                       'bootstrap':[True,False]\n",
        "},cv=5,return_train_score=False,n_jobs=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DciVPQYefBsm",
        "outputId": "bf6b071e-8ba0-40b7-f1f7-41653c4d8567",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "source": [
        "gscv_rf.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
              "                                              class_weight=None,\n",
              "                                              criterion='gini', max_depth=None,\n",
              "                                              max_features='auto',\n",
              "                                              max_leaf_nodes=None,\n",
              "                                              max_samples=None,\n",
              "                                              min_impurity_decrease=0.0,\n",
              "                                              min_impurity_split=None,\n",
              "                                              min_samples_leaf=1,\n",
              "                                              min_samples_split=2,\n",
              "                                              min_weight_fraction_leaf=0.0,\n",
              "                                              n_estimators=100, n_jobs=None,...\n",
              "                                              warm_start=False),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid={'bootstrap': [True, False],\n",
              "                         'ccp_alpha': [0.2, 0.05, 1.5, 2.5],\n",
              "                         'class_weight': [{0: 0.5, 1: 0.5}, {0: 0.6, 1: 0.4}],\n",
              "                         'criterion': ['gini', 'entropy'],\n",
              "                         'max_depth': [5, 10, 15, 20, 25, 35, 52, 85, 100],\n",
              "                         'n_estimators': [1, 10, 15, 20, 52, 60, 100]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdbmedRkfBvB",
        "outputId": "8d632277-349a-437c-e8d7-ddfc691300d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "gscv_rf.best_score_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.837433155080214"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Duu5m7tfByT",
        "outputId": "36c95e04-fab5-4021-c28a-e91fae9fcaff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "gscv_rf.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bootstrap': False,\n",
              " 'ccp_alpha': 0.05,\n",
              " 'class_weight': {0: 0.5, 1: 0.5},\n",
              " 'criterion': 'entropy',\n",
              " 'max_depth': 52,\n",
              " 'n_estimators': 20}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPWkUvVPlE2W",
        "outputId": "1c014081-2b4c-465a-b638-2e2c10e5aa73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_test_pred_gridrf = gscv_rf.predict(X_test)\n",
        "accuracy_test = accuracy_score(y_test, y_test_pred_gridrf)\n",
        "print(\"Accuracy:\"+str(accuracy_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:0.8571428571428571\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwGn7qMnTeG0"
      },
      "source": [
        "# Performing Neural Networks on the Dataset\n",
        "# Importing the Necessary Libraries for Neural Networks\n",
        "from keras import Sequential\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Dense\n",
        "import tensorflow\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZ_pyiHuX9M2"
      },
      "source": [
        "y_train=to_categorical(y_train)\n",
        "y_test=to_categorical(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20TH4slzYFbv",
        "outputId": "999e0d7f-2b86-4964-ba09-d0bbd9f0f91b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(166, 60)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QE4jVuteYKGm",
        "outputId": "c9e21117-9126-48a9-bada-bf9d8127d589",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(166, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crhsuRqYYMUP",
        "outputId": "46676026-8882-4edf-d63a-e64d5b9cfc1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Performing Neural Network on X_train and Y_train\n",
        "model=Sequential()\n",
        "model.add(Dense(units=580, activation='relu', input_dim=60))\n",
        "model.add(Dense(units=2800, activation='relu'))\n",
        "model.add(Dense(units=1041, activation='relu'))\n",
        "model.add(Dense(units=2041))\n",
        "model.add(LeakyReLU(alpha=0.05))\n",
        "model.add(Dense(units=2, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "history1=model.fit(X_train, y_train, epochs=100,validation_split=0.2, verbose=1,batch_size=128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "2/2 [==============================] - 0s 219ms/step - loss: 0.7039 - accuracy: 0.4697 - val_loss: 0.7811 - val_accuracy: 0.4412\n",
            "Epoch 2/100\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.7399 - accuracy: 0.5000 - val_loss: 0.7158 - val_accuracy: 0.4412\n",
            "Epoch 3/100\n",
            "2/2 [==============================] - 0s 77ms/step - loss: 0.6930 - accuracy: 0.5000 - val_loss: 0.7043 - val_accuracy: 0.4412\n",
            "Epoch 4/100\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.6742 - accuracy: 0.5000 - val_loss: 0.6854 - val_accuracy: 0.4706\n",
            "Epoch 5/100\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.6469 - accuracy: 0.6667 - val_loss: 0.6724 - val_accuracy: 0.5588\n",
            "Epoch 6/100\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.6670 - accuracy: 0.5758 - val_loss: 0.6795 - val_accuracy: 0.5588\n",
            "Epoch 7/100\n",
            "2/2 [==============================] - 0s 79ms/step - loss: 0.6980 - accuracy: 0.5000 - val_loss: 0.6689 - val_accuracy: 0.5588\n",
            "Epoch 8/100\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.6781 - accuracy: 0.5152 - val_loss: 0.6572 - val_accuracy: 0.6176\n",
            "Epoch 9/100\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.6433 - accuracy: 0.6061 - val_loss: 0.6265 - val_accuracy: 0.6471\n",
            "Epoch 10/100\n",
            "2/2 [==============================] - 0s 71ms/step - loss: 0.5878 - accuracy: 0.7803 - val_loss: 0.6456 - val_accuracy: 0.5588\n",
            "Epoch 11/100\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.5792 - accuracy: 0.6818 - val_loss: 0.5318 - val_accuracy: 0.6765\n",
            "Epoch 12/100\n",
            "2/2 [==============================] - 0s 75ms/step - loss: 0.4811 - accuracy: 0.8106 - val_loss: 0.4961 - val_accuracy: 0.7647\n",
            "Epoch 13/100\n",
            "2/2 [==============================] - 0s 71ms/step - loss: 0.5008 - accuracy: 0.7197 - val_loss: 0.5257 - val_accuracy: 0.7647\n",
            "Epoch 14/100\n",
            "2/2 [==============================] - 0s 73ms/step - loss: 0.5652 - accuracy: 0.6742 - val_loss: 0.4341 - val_accuracy: 0.7941\n",
            "Epoch 15/100\n",
            "2/2 [==============================] - 0s 71ms/step - loss: 0.4390 - accuracy: 0.7652 - val_loss: 0.6281 - val_accuracy: 0.7059\n",
            "Epoch 16/100\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.5120 - accuracy: 0.7197 - val_loss: 0.6779 - val_accuracy: 0.6765\n",
            "Epoch 17/100\n",
            "2/2 [==============================] - 0s 73ms/step - loss: 0.5253 - accuracy: 0.6970 - val_loss: 0.4092 - val_accuracy: 0.7941\n",
            "Epoch 18/100\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.3632 - accuracy: 0.8712 - val_loss: 0.4442 - val_accuracy: 0.7647\n",
            "Epoch 19/100\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.4320 - accuracy: 0.7652 - val_loss: 0.4248 - val_accuracy: 0.7941\n",
            "Epoch 20/100\n",
            "2/2 [==============================] - 0s 74ms/step - loss: 0.3820 - accuracy: 0.8333 - val_loss: 0.4907 - val_accuracy: 0.7353\n",
            "Epoch 21/100\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.4002 - accuracy: 0.8258 - val_loss: 0.6844 - val_accuracy: 0.6176\n",
            "Epoch 22/100\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.5117 - accuracy: 0.7121 - val_loss: 0.6009 - val_accuracy: 0.7353\n",
            "Epoch 23/100\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.4215 - accuracy: 0.8030 - val_loss: 0.4615 - val_accuracy: 0.7059\n",
            "Epoch 24/100\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.3279 - accuracy: 0.8712 - val_loss: 0.4415 - val_accuracy: 0.6765\n",
            "Epoch 25/100\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.3211 - accuracy: 0.8182 - val_loss: 0.4417 - val_accuracy: 0.6765\n",
            "Epoch 26/100\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.2795 - accuracy: 0.8636 - val_loss: 0.4881 - val_accuracy: 0.7059\n",
            "Epoch 27/100\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.2740 - accuracy: 0.9015 - val_loss: 0.3824 - val_accuracy: 0.7059\n",
            "Epoch 28/100\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.2342 - accuracy: 0.8939 - val_loss: 0.4625 - val_accuracy: 0.7647\n",
            "Epoch 29/100\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.3141 - accuracy: 0.8258 - val_loss: 0.5164 - val_accuracy: 0.7647\n",
            "Epoch 30/100\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 0.3348 - accuracy: 0.8333 - val_loss: 0.3692 - val_accuracy: 0.7059\n",
            "Epoch 31/100\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1711 - accuracy: 0.9091 - val_loss: 0.6764 - val_accuracy: 0.7353\n",
            "Epoch 32/100\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.2850 - accuracy: 0.8864 - val_loss: 0.3821 - val_accuracy: 0.7647\n",
            "Epoch 33/100\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1286 - accuracy: 0.9470 - val_loss: 0.4271 - val_accuracy: 0.8235\n",
            "Epoch 34/100\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.2028 - accuracy: 0.9015 - val_loss: 0.4459 - val_accuracy: 0.8529\n",
            "Epoch 35/100\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.2131 - accuracy: 0.8939 - val_loss: 0.4531 - val_accuracy: 0.7941\n",
            "Epoch 36/100\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.1059 - accuracy: 0.9621 - val_loss: 0.4040 - val_accuracy: 0.8235\n",
            "Epoch 37/100\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.0839 - accuracy: 0.9621 - val_loss: 0.3794 - val_accuracy: 0.8235\n",
            "Epoch 38/100\n",
            "2/2 [==============================] - 0s 73ms/step - loss: 0.1165 - accuracy: 0.9470 - val_loss: 0.4760 - val_accuracy: 0.8529\n",
            "Epoch 39/100\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.1230 - accuracy: 0.9621 - val_loss: 0.6171 - val_accuracy: 0.7941\n",
            "Epoch 40/100\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1457 - accuracy: 0.9242 - val_loss: 0.5480 - val_accuracy: 0.8235\n",
            "Epoch 41/100\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.1530 - accuracy: 0.9394 - val_loss: 0.8444 - val_accuracy: 0.7647\n",
            "Epoch 42/100\n",
            "2/2 [==============================] - 0s 73ms/step - loss: 0.4943 - accuracy: 0.7803 - val_loss: 0.6773 - val_accuracy: 0.7647\n",
            "Epoch 43/100\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.3018 - accuracy: 0.8636 - val_loss: 0.3612 - val_accuracy: 0.7941\n",
            "Epoch 44/100\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.1302 - accuracy: 0.9621 - val_loss: 0.6371 - val_accuracy: 0.6471\n",
            "Epoch 45/100\n",
            "2/2 [==============================] - 0s 73ms/step - loss: 0.2801 - accuracy: 0.8333 - val_loss: 0.4426 - val_accuracy: 0.7941\n",
            "Epoch 46/100\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.1271 - accuracy: 0.9621 - val_loss: 0.3734 - val_accuracy: 0.8235\n",
            "Epoch 47/100\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.0911 - accuracy: 0.9621 - val_loss: 0.4315 - val_accuracy: 0.7941\n",
            "Epoch 48/100\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.0966 - accuracy: 0.9621 - val_loss: 0.3803 - val_accuracy: 0.8235\n",
            "Epoch 49/100\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.0520 - accuracy: 0.9848 - val_loss: 0.4563 - val_accuracy: 0.8235\n",
            "Epoch 50/100\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.0362 - accuracy: 0.9924 - val_loss: 0.6725 - val_accuracy: 0.7941\n",
            "Epoch 51/100\n",
            "2/2 [==============================] - 0s 78ms/step - loss: 0.0593 - accuracy: 0.9621 - val_loss: 0.5747 - val_accuracy: 0.8529\n",
            "Epoch 52/100\n",
            "2/2 [==============================] - 0s 79ms/step - loss: 0.0235 - accuracy: 1.0000 - val_loss: 0.5509 - val_accuracy: 0.8529\n",
            "Epoch 53/100\n",
            "2/2 [==============================] - 0s 71ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.6544 - val_accuracy: 0.7647\n",
            "Epoch 54/100\n",
            "2/2 [==============================] - 0s 71ms/step - loss: 0.0172 - accuracy: 0.9924 - val_loss: 0.5968 - val_accuracy: 0.8235\n",
            "Epoch 55/100\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.5564 - val_accuracy: 0.7941\n",
            "Epoch 56/100\n",
            "2/2 [==============================] - 0s 79ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.5594 - val_accuracy: 0.8529\n",
            "Epoch 57/100\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6227 - val_accuracy: 0.8824\n",
            "Epoch 58/100\n",
            "2/2 [==============================] - 0s 74ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6671 - val_accuracy: 0.8824\n",
            "Epoch 59/100\n",
            "2/2 [==============================] - 0s 77ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6668 - val_accuracy: 0.8824\n",
            "Epoch 60/100\n",
            "2/2 [==============================] - 0s 78ms/step - loss: 9.8914e-04 - accuracy: 1.0000 - val_loss: 0.6350 - val_accuracy: 0.8824\n",
            "Epoch 61/100\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 5.1481e-04 - accuracy: 1.0000 - val_loss: 0.6045 - val_accuracy: 0.8529\n",
            "Epoch 62/100\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 2.7136e-04 - accuracy: 1.0000 - val_loss: 0.5915 - val_accuracy: 0.8529\n",
            "Epoch 63/100\n",
            "2/2 [==============================] - 0s 74ms/step - loss: 1.6740e-04 - accuracy: 1.0000 - val_loss: 0.5958 - val_accuracy: 0.8235\n",
            "Epoch 64/100\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 1.2059e-04 - accuracy: 1.0000 - val_loss: 0.6144 - val_accuracy: 0.8529\n",
            "Epoch 65/100\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 1.0233e-04 - accuracy: 1.0000 - val_loss: 0.6455 - val_accuracy: 0.8235\n",
            "Epoch 66/100\n",
            "2/2 [==============================] - 0s 78ms/step - loss: 1.0420e-04 - accuracy: 1.0000 - val_loss: 0.6807 - val_accuracy: 0.8235\n",
            "Epoch 67/100\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 1.1014e-04 - accuracy: 1.0000 - val_loss: 0.7081 - val_accuracy: 0.8235\n",
            "Epoch 68/100\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.0653e-04 - accuracy: 1.0000 - val_loss: 0.7247 - val_accuracy: 0.8235\n",
            "Epoch 69/100\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 9.0144e-05 - accuracy: 1.0000 - val_loss: 0.7321 - val_accuracy: 0.8235\n",
            "Epoch 70/100\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 6.9135e-05 - accuracy: 1.0000 - val_loss: 0.7323 - val_accuracy: 0.8235\n",
            "Epoch 71/100\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 4.6961e-05 - accuracy: 1.0000 - val_loss: 0.7094 - val_accuracy: 0.8235\n",
            "Epoch 72/100\n",
            "2/2 [==============================] - 0s 76ms/step - loss: 2.9159e-05 - accuracy: 1.0000 - val_loss: 0.6791 - val_accuracy: 0.8529\n",
            "Epoch 73/100\n",
            "2/2 [==============================] - 0s 82ms/step - loss: 2.0273e-05 - accuracy: 1.0000 - val_loss: 0.6649 - val_accuracy: 0.8529\n",
            "Epoch 74/100\n",
            "2/2 [==============================] - 0s 74ms/step - loss: 1.8903e-05 - accuracy: 1.0000 - val_loss: 0.6623 - val_accuracy: 0.8824\n",
            "Epoch 75/100\n",
            "2/2 [==============================] - 0s 75ms/step - loss: 1.9599e-05 - accuracy: 1.0000 - val_loss: 0.6666 - val_accuracy: 0.9118\n",
            "Epoch 76/100\n",
            "2/2 [==============================] - 0s 78ms/step - loss: 2.1006e-05 - accuracy: 1.0000 - val_loss: 0.6744 - val_accuracy: 0.8824\n",
            "Epoch 77/100\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 2.2602e-05 - accuracy: 1.0000 - val_loss: 0.6829 - val_accuracy: 0.8824\n",
            "Epoch 78/100\n",
            "2/2 [==============================] - 0s 81ms/step - loss: 2.3979e-05 - accuracy: 1.0000 - val_loss: 0.6888 - val_accuracy: 0.8824\n",
            "Epoch 79/100\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 2.2522e-05 - accuracy: 1.0000 - val_loss: 0.6920 - val_accuracy: 0.8824\n",
            "Epoch 80/100\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 1.9870e-05 - accuracy: 1.0000 - val_loss: 0.6949 - val_accuracy: 0.8824\n",
            "Epoch 81/100\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 1.7780e-05 - accuracy: 1.0000 - val_loss: 0.6975 - val_accuracy: 0.8824\n",
            "Epoch 82/100\n",
            "2/2 [==============================] - 0s 75ms/step - loss: 1.6107e-05 - accuracy: 1.0000 - val_loss: 0.6998 - val_accuracy: 0.9118\n",
            "Epoch 83/100\n",
            "2/2 [==============================] - 0s 74ms/step - loss: 1.4744e-05 - accuracy: 1.0000 - val_loss: 0.7019 - val_accuracy: 0.9118\n",
            "Epoch 84/100\n",
            "2/2 [==============================] - 0s 74ms/step - loss: 1.3568e-05 - accuracy: 1.0000 - val_loss: 0.7038 - val_accuracy: 0.9118\n",
            "Epoch 85/100\n",
            "2/2 [==============================] - 0s 79ms/step - loss: 1.2531e-05 - accuracy: 1.0000 - val_loss: 0.7058 - val_accuracy: 0.9118\n",
            "Epoch 86/100\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 1.1489e-05 - accuracy: 1.0000 - val_loss: 0.7078 - val_accuracy: 0.9118\n",
            "Epoch 87/100\n",
            "2/2 [==============================] - 0s 75ms/step - loss: 1.0653e-05 - accuracy: 1.0000 - val_loss: 0.7098 - val_accuracy: 0.9118\n",
            "Epoch 88/100\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 9.9712e-06 - accuracy: 1.0000 - val_loss: 0.7118 - val_accuracy: 0.9118\n",
            "Epoch 89/100\n",
            "2/2 [==============================] - 0s 77ms/step - loss: 9.4019e-06 - accuracy: 1.0000 - val_loss: 0.7137 - val_accuracy: 0.9118\n",
            "Epoch 90/100\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 8.9286e-06 - accuracy: 1.0000 - val_loss: 0.7154 - val_accuracy: 0.9118\n",
            "Epoch 91/100\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 8.5293e-06 - accuracy: 1.0000 - val_loss: 0.7172 - val_accuracy: 0.9118\n",
            "Epoch 92/100\n",
            "2/2 [==============================] - 0s 74ms/step - loss: 8.1842e-06 - accuracy: 1.0000 - val_loss: 0.7191 - val_accuracy: 0.9118\n",
            "Epoch 93/100\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 7.8830e-06 - accuracy: 1.0000 - val_loss: 0.7207 - val_accuracy: 0.9118\n",
            "Epoch 94/100\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 7.6183e-06 - accuracy: 1.0000 - val_loss: 0.7222 - val_accuracy: 0.9118\n",
            "Epoch 95/100\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 7.3831e-06 - accuracy: 1.0000 - val_loss: 0.7236 - val_accuracy: 0.9118\n",
            "Epoch 96/100\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 7.1715e-06 - accuracy: 1.0000 - val_loss: 0.7249 - val_accuracy: 0.9118\n",
            "Epoch 97/100\n",
            "2/2 [==============================] - 0s 75ms/step - loss: 6.9789e-06 - accuracy: 1.0000 - val_loss: 0.7262 - val_accuracy: 0.9118\n",
            "Epoch 98/100\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 6.8019e-06 - accuracy: 1.0000 - val_loss: 0.7276 - val_accuracy: 0.9118\n",
            "Epoch 99/100\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 6.6339e-06 - accuracy: 1.0000 - val_loss: 0.7289 - val_accuracy: 0.9118\n",
            "Epoch 100/100\n",
            "2/2 [==============================] - 0s 76ms/step - loss: 6.4744e-06 - accuracy: 1.0000 - val_loss: 0.7302 - val_accuracy: 0.9118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xkE_HaIYUpS",
        "outputId": "1322bfd2-9145-4949-fbbd-ba0a4d4b1391",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "model.evaluate(X_test,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 10ms/step - loss: 2.1925 - accuracy: 0.7381\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.1925196647644043, 0.738095223903656]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA_mW1WFbve6"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history1.history['accuracy'])\n",
        "plt.plot(history1.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history1.history['loss'])\n",
        "plt.plot(history1.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azZ9UrClZ9nm"
      },
      "source": [
        "**2nd Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VetmvM-x9moi"
      },
      "source": [
        "# 2nd Dataset\n",
        "names=['Wife  age','Wife education','Husband education','Number of children ever born','Wife religion','Wife now working?','Husband occupation','Standard-of-living index','Media exposure','Contraceptive method used']\n",
        "df3=pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/cmc/cmc.data',names=names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJg8GPAl_Qef"
      },
      "source": [
        " Attribute Information:\n",
        "\n",
        "   1. Wife's age                     (numerical)\n",
        "   2. Wife's education               (categorical)      1=low, 2, 3, 4=high\n",
        "   3. Husband's education            (categorical)      1=low, 2, 3, 4=high\n",
        "   4. Number of children ever born   (numerical)\n",
        "   5. Wife's religion                (binary)           0=Non-Islam, 1=Islam\n",
        "   6. Wife's now working?            (binary)           0=Yes, 1=No\n",
        "   7. Husband's occupation           (categorical)      1, 2, 3, 4\n",
        "   8. Standard-of-living index       (categorical)      1=low, 2, 3, 4=high\n",
        "   9. Media exposure                 (binary)           0=Good, 1=Not good\n",
        "   10. Contraceptive method used     (class attribute)  1=No-use,2=Long-term,3=Short-term\n",
        "                                                        \n",
        "                                                        "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "790gTA2V9mth",
        "outputId": "5a04f37e-afb3-4d51-bf55-81110e9429e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "df3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Wife  age</th>\n",
              "      <th>Wife education</th>\n",
              "      <th>Husband education</th>\n",
              "      <th>Number of children ever born</th>\n",
              "      <th>Wife religion</th>\n",
              "      <th>Wife now working?</th>\n",
              "      <th>Husband occupation</th>\n",
              "      <th>Standard-of-living index</th>\n",
              "      <th>Media exposure</th>\n",
              "      <th>Contraceptive method used</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>45</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>43</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>42</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>36</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1468</th>\n",
              "      <td>33</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1469</th>\n",
              "      <td>33</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1470</th>\n",
              "      <td>39</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1471</th>\n",
              "      <td>33</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1472</th>\n",
              "      <td>17</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1473 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Wife  age  Wife education  ...  Media exposure  Contraceptive method used\n",
              "0            24               2  ...               0                          1\n",
              "1            45               1  ...               0                          1\n",
              "2            43               2  ...               0                          1\n",
              "3            42               3  ...               0                          1\n",
              "4            36               3  ...               0                          1\n",
              "...         ...             ...  ...             ...                        ...\n",
              "1468         33               4  ...               0                          3\n",
              "1469         33               4  ...               0                          3\n",
              "1470         39               3  ...               0                          3\n",
              "1471         33               3  ...               0                          3\n",
              "1472         17               3  ...               0                          3\n",
              "\n",
              "[1473 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5GF671P9mw3",
        "outputId": "2367dd65-9084-4972-fae0-f371bdf61d8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "# Checking out for the Missing values\n",
        "df3.isna().sum()/len(df3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Wife  age                       0.0\n",
              "Wife education                  0.0\n",
              "Husband education               0.0\n",
              "Number of children ever born    0.0\n",
              "Wife religion                   0.0\n",
              "Wife now working?               0.0\n",
              "Husband occupation              0.0\n",
              "Standard-of-living index        0.0\n",
              "Media exposure                  0.0\n",
              "Contraceptive method used       0.0\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy0ALHu19mrE",
        "outputId": "2be2c855-6e9b-452b-eb98-534c27b8a506",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "sns.countplot(df3['Contraceptive method used'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cx6SKx-j9mnX"
      },
      "source": [
        "target_3=df3['Contraceptive method used']\n",
        "df3.drop(columns={'Contraceptive method used'},inplace=True)\n",
        "df3=pd.DataFrame(m.fit_transform(df3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uLz20ux9mlj",
        "outputId": "6af172a9-5842-4d87-ad48-a2dddaa1b402",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "df3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.242424</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.1875</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.878788</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.6250</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.818182</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.4375</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.787879</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.5625</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.606061</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.5000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1468</th>\n",
              "      <td>0.515152</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.1250</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1469</th>\n",
              "      <td>0.515152</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.1875</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1470</th>\n",
              "      <td>0.696970</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.5000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1471</th>\n",
              "      <td>0.515152</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.2500</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1472</th>\n",
              "      <td>0.030303</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.0625</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1473 rows Ã— 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0         1         2       3    4    5         6         7    8\n",
              "0     0.242424  0.333333  0.666667  0.1875  1.0  1.0  0.333333  0.666667  0.0\n",
              "1     0.878788  0.000000  0.666667  0.6250  1.0  1.0  0.666667  1.000000  0.0\n",
              "2     0.818182  0.333333  0.666667  0.4375  1.0  1.0  0.666667  1.000000  0.0\n",
              "3     0.787879  0.666667  0.333333  0.5625  1.0  1.0  0.666667  0.666667  0.0\n",
              "4     0.606061  0.666667  0.666667  0.5000  1.0  1.0  0.666667  0.333333  0.0\n",
              "...        ...       ...       ...     ...  ...  ...       ...       ...  ...\n",
              "1468  0.515152  1.000000  1.000000  0.1250  1.0  0.0  0.333333  1.000000  0.0\n",
              "1469  0.515152  1.000000  1.000000  0.1875  1.0  1.0  0.000000  1.000000  0.0\n",
              "1470  0.696970  0.666667  0.666667  0.5000  1.0  0.0  0.000000  1.000000  0.0\n",
              "1471  0.515152  0.666667  0.666667  0.2500  1.0  0.0  0.333333  0.333333  0.0\n",
              "1472  0.030303  0.666667  0.666667  0.0625  1.0  1.0  0.333333  1.000000  0.0\n",
              "\n",
              "[1473 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOHqEz96JNWe"
      },
      "source": [
        "target_3=ln.fit_transform(target_3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge-RYTbAIkHA"
      },
      "source": [
        "# Splitting into trainset and testset in the ratio 80:20\n",
        "seed=8\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train2,X_test2,y_train2,y_test2=train_test_split(df3,target_3,test_size=0.2,random_state=seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmHhLENOItxQ"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "scores_log_3=cross_val_score(logistic,X_train2,y_train2,cv=3,scoring='accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVJWMVzQIyvS",
        "outputId": "7e1378ba-f031-4dbb-cb4d-998c3a60fd8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.mean(scores_log_3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5118744698897371"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Vbi-ocfI0l-"
      },
      "source": [
        "scores_sgd_3=cross_val_score(sgd,X_train2,y_train2,cv=3,scoring='accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4J4Vf3yNI6rI",
        "outputId": "c27647aa-a232-4b12-861f-241a9d5dfdec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.mean(scores_sgd_3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4838997940142978"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrsSyVAkI88t"
      },
      "source": [
        "# GridSearch CV on Logistic Regression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "gscv_log=GridSearchCV(logistic, {\n",
        "                      'penalty': ['l1','l2','elasticnet'],\n",
        "                      'C': [0.1,0.5,0.25,0.8,0.9,0.95,1.5,2.3,5.5],\n",
        "                      'class_weight' : [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}],\n",
        "                      'solver' : ['liblinear', 'saga']\n",
        "                      },cv=3,return_train_score=False,n_jobs=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5MklkHWJDuG",
        "outputId": "513b1f7b-7a14-4d77-a791-2054a1a77f63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "gscv_log.fit(X_train2,y_train2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=3, error_score=nan,\n",
              "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                          fit_intercept=True,\n",
              "                                          intercept_scaling=1, l1_ratio=None,\n",
              "                                          max_iter=100, multi_class='auto',\n",
              "                                          n_jobs=None, penalty='l2',\n",
              "                                          random_state=None, solver='lbfgs',\n",
              "                                          tol=0.0001, verbose=0,\n",
              "                                          warm_start=False),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid={'C': [0.1, 0.5, 0.25, 0.8, 0.9, 0.95, 1.5, 2.3, 5.5],\n",
              "                         'class_weight': [{0: 0.5, 1: 0.5}, {0: 0.6, 1: 0.4}],\n",
              "                         'penalty': ['l1', 'l2', 'elasticnet'],\n",
              "                         'solver': ['liblinear', 'saga']},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PV5jJlTZJG_O",
        "outputId": "d756a5af-7ff9-4ba5-f75f-ef2621924f43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "gscv_log.best_score_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5203713801042045"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAUotwbcJLQX",
        "outputId": "6f9eccf2-1a33-4c37-b6e6-43365adb9988",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "gscv_log.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'C': 5.5,\n",
              " 'class_weight': {0: 0.5, 1: 0.5},\n",
              " 'penalty': 'l2',\n",
              " 'solver': 'liblinear'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZfxuVd_JbAU",
        "outputId": "20eefefa-624c-4d51-fab3-f036221575c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Accuracy on the Test Set\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_test_pred2 = gscv_log.predict(X_test2)\n",
        "accuracy_test = accuracy_score(y_test2, y_test_pred2)\n",
        "print(\"Accuracy:\"+str(accuracy_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:0.4915254237288136\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5D18B69La9j"
      },
      "source": [
        "# Performing Neural Networks on the Dataset\n",
        "# Importing the Necessary Libraries for Neural Networks\n",
        "from keras import Sequential\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Dense\n",
        "import tensorflow\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujxD0Ly_Jxxs"
      },
      "source": [
        "y_train2=to_categorical(y_train2)\n",
        "y_test2=to_categorical(y_test2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nHxMvaqLaBE",
        "outputId": "85941b1a-9236-425b-de03-d1f8cdcace5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_train2.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1178, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OCXZf89Lk01",
        "outputId": "d78a468f-bc22-488a-9b8f-81d38807519b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_train2.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1178, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsO7GmT5Lk_3",
        "outputId": "07ea9f21-b435-47dd-fc59-7446e929703e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Performing Neural Network on X_train and Y_train\n",
        "model1=Sequential()\n",
        "model1.add(Dense(units=580, activation='relu', input_dim=9))\n",
        "model1.add(Dense(units=2800, activation='relu'))\n",
        "model1.add(Dense(units=1041, activation='relu'))\n",
        "#model1.add(Dense(units=2041))\n",
        "#model1.add(LeakyReLU(alpha=0.05))\n",
        "model1.add(Dense(units=3, activation='softmax'))\n",
        "model1.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "history2=model1.fit(X_train2, y_train2, epochs=100,validation_split=0.2, verbose=1,batch_size=128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 104ms/step - loss: 1.0578 - accuracy: 0.4119 - val_loss: 0.9905 - val_accuracy: 0.4746\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 1s 84ms/step - loss: 0.9884 - accuracy: 0.4820 - val_loss: 0.9909 - val_accuracy: 0.4619\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 1s 86ms/step - loss: 0.9627 - accuracy: 0.5180 - val_loss: 0.9654 - val_accuracy: 0.4958\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 1s 86ms/step - loss: 0.9208 - accuracy: 0.5626 - val_loss: 1.0292 - val_accuracy: 0.5042\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 1s 85ms/step - loss: 0.9097 - accuracy: 0.5510 - val_loss: 0.9778 - val_accuracy: 0.5085\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 1s 88ms/step - loss: 0.9168 - accuracy: 0.5594 - val_loss: 0.9868 - val_accuracy: 0.4958\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 1s 83ms/step - loss: 0.9083 - accuracy: 0.5648 - val_loss: 0.9816 - val_accuracy: 0.4915\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 1s 80ms/step - loss: 0.8956 - accuracy: 0.5839 - val_loss: 1.0226 - val_accuracy: 0.5297\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 1s 80ms/step - loss: 0.8773 - accuracy: 0.5860 - val_loss: 1.0531 - val_accuracy: 0.5297\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 1s 82ms/step - loss: 0.8461 - accuracy: 0.5998 - val_loss: 1.0357 - val_accuracy: 0.5254\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 1s 82ms/step - loss: 0.8344 - accuracy: 0.6040 - val_loss: 1.0720 - val_accuracy: 0.5381\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 1s 83ms/step - loss: 0.8271 - accuracy: 0.6136 - val_loss: 0.9685 - val_accuracy: 0.5042\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 1s 82ms/step - loss: 0.8238 - accuracy: 0.5955 - val_loss: 0.9757 - val_accuracy: 0.5424\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 1s 82ms/step - loss: 0.8368 - accuracy: 0.6030 - val_loss: 0.9796 - val_accuracy: 0.5297\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 1s 82ms/step - loss: 0.7938 - accuracy: 0.6401 - val_loss: 1.0143 - val_accuracy: 0.5424\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 1s 79ms/step - loss: 0.7898 - accuracy: 0.6306 - val_loss: 1.0102 - val_accuracy: 0.5424\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 1s 80ms/step - loss: 0.7737 - accuracy: 0.6316 - val_loss: 1.0448 - val_accuracy: 0.5212\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 1s 82ms/step - loss: 0.7725 - accuracy: 0.6338 - val_loss: 1.0143 - val_accuracy: 0.5339\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 1s 81ms/step - loss: 0.7634 - accuracy: 0.6359 - val_loss: 1.0346 - val_accuracy: 0.5466\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 1s 81ms/step - loss: 0.7699 - accuracy: 0.6348 - val_loss: 1.0562 - val_accuracy: 0.5127\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 1s 82ms/step - loss: 0.7464 - accuracy: 0.6667 - val_loss: 1.0905 - val_accuracy: 0.5508\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 1s 80ms/step - loss: 0.7315 - accuracy: 0.6539 - val_loss: 1.1407 - val_accuracy: 0.5254\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 1s 82ms/step - loss: 0.7084 - accuracy: 0.6752 - val_loss: 1.2023 - val_accuracy: 0.5466\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 1s 80ms/step - loss: 0.6950 - accuracy: 0.6805 - val_loss: 1.1077 - val_accuracy: 0.5127\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 1s 82ms/step - loss: 0.7047 - accuracy: 0.6826 - val_loss: 1.2023 - val_accuracy: 0.5508\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 1s 81ms/step - loss: 0.6834 - accuracy: 0.6868 - val_loss: 1.1533 - val_accuracy: 0.5466\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 1s 82ms/step - loss: 0.6730 - accuracy: 0.6953 - val_loss: 1.2413 - val_accuracy: 0.5042\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 1s 80ms/step - loss: 0.6608 - accuracy: 0.6964 - val_loss: 1.3954 - val_accuracy: 0.5339\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 1s 81ms/step - loss: 0.6529 - accuracy: 0.7038 - val_loss: 1.2435 - val_accuracy: 0.5042\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 1s 81ms/step - loss: 0.6320 - accuracy: 0.7208 - val_loss: 1.3048 - val_accuracy: 0.5169\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 1s 80ms/step - loss: 0.6379 - accuracy: 0.7049 - val_loss: 1.3542 - val_accuracy: 0.5297\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 1s 81ms/step - loss: 0.6888 - accuracy: 0.6815 - val_loss: 1.4137 - val_accuracy: 0.5254\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 1s 82ms/step - loss: 0.6499 - accuracy: 0.6900 - val_loss: 1.3699 - val_accuracy: 0.5466\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 1s 81ms/step - loss: 0.6170 - accuracy: 0.7166 - val_loss: 1.3589 - val_accuracy: 0.5254\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 1s 81ms/step - loss: 0.6230 - accuracy: 0.7102 - val_loss: 1.3729 - val_accuracy: 0.5254\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 1s 80ms/step - loss: 0.6000 - accuracy: 0.7335 - val_loss: 1.3777 - val_accuracy: 0.5085\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 1s 80ms/step - loss: 0.5995 - accuracy: 0.7219 - val_loss: 1.4856 - val_accuracy: 0.4958\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 1s 81ms/step - loss: 0.6058 - accuracy: 0.7229 - val_loss: 1.4138 - val_accuracy: 0.5127\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 1s 81ms/step - loss: 0.5906 - accuracy: 0.7335 - val_loss: 1.4878 - val_accuracy: 0.5381\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 1s 84ms/step - loss: 0.5746 - accuracy: 0.7367 - val_loss: 1.4449 - val_accuracy: 0.4873\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 1s 85ms/step - loss: 0.5776 - accuracy: 0.7473 - val_loss: 1.4943 - val_accuracy: 0.5042\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 1s 80ms/step - loss: 0.5873 - accuracy: 0.7293 - val_loss: 1.5886 - val_accuracy: 0.4915\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 1s 81ms/step - loss: 0.5963 - accuracy: 0.7272 - val_loss: 1.4968 - val_accuracy: 0.5000\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 1s 80ms/step - loss: 0.5639 - accuracy: 0.7410 - val_loss: 1.6644 - val_accuracy: 0.5297\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 1s 83ms/step - loss: 0.5505 - accuracy: 0.7622 - val_loss: 1.5979 - val_accuracy: 0.5212\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 1s 82ms/step - loss: 0.5272 - accuracy: 0.7696 - val_loss: 1.5953 - val_accuracy: 0.5127\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 1s 81ms/step - loss: 0.5372 - accuracy: 0.7611 - val_loss: 1.7013 - val_accuracy: 0.5000\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 1s 81ms/step - loss: 0.5327 - accuracy: 0.7622 - val_loss: 1.7179 - val_accuracy: 0.5127\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 1s 80ms/step - loss: 0.5140 - accuracy: 0.7718 - val_loss: 1.7425 - val_accuracy: 0.4915\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 1s 85ms/step - loss: 0.4971 - accuracy: 0.7771 - val_loss: 1.8819 - val_accuracy: 0.5381\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 1s 83ms/step - loss: 0.4972 - accuracy: 0.7749 - val_loss: 1.8930 - val_accuracy: 0.5000\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 1s 83ms/step - loss: 0.4948 - accuracy: 0.7813 - val_loss: 1.9522 - val_accuracy: 0.5085\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 1s 86ms/step - loss: 0.4840 - accuracy: 0.7866 - val_loss: 1.8888 - val_accuracy: 0.5000\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 1s 86ms/step - loss: 0.4700 - accuracy: 0.7919 - val_loss: 1.8908 - val_accuracy: 0.5000\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 1s 88ms/step - loss: 0.4634 - accuracy: 0.7909 - val_loss: 2.1806 - val_accuracy: 0.4958\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 1s 81ms/step - loss: 0.4766 - accuracy: 0.7866 - val_loss: 2.0311 - val_accuracy: 0.5042\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 1s 134ms/step - loss: 0.4548 - accuracy: 0.7972 - val_loss: 1.9616 - val_accuracy: 0.4873\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 1s 134ms/step - loss: 0.4370 - accuracy: 0.8100 - val_loss: 2.2028 - val_accuracy: 0.5000\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 1s 130ms/step - loss: 0.4271 - accuracy: 0.8047 - val_loss: 2.1068 - val_accuracy: 0.5000\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 1s 129ms/step - loss: 0.4252 - accuracy: 0.8121 - val_loss: 2.3010 - val_accuracy: 0.5169\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 1s 130ms/step - loss: 0.4288 - accuracy: 0.8121 - val_loss: 2.3622 - val_accuracy: 0.5000\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 1s 129ms/step - loss: 0.4531 - accuracy: 0.7951 - val_loss: 2.3483 - val_accuracy: 0.5127\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 1s 133ms/step - loss: 0.4345 - accuracy: 0.8153 - val_loss: 2.3724 - val_accuracy: 0.4915\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 1s 137ms/step - loss: 0.4703 - accuracy: 0.7930 - val_loss: 2.4478 - val_accuracy: 0.5212\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 1s 132ms/step - loss: 0.4398 - accuracy: 0.8089 - val_loss: 2.1815 - val_accuracy: 0.5000\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 0.4260 - accuracy: 0.8047 - val_loss: 2.4581 - val_accuracy: 0.4958\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 1s 132ms/step - loss: 0.4199 - accuracy: 0.8280 - val_loss: 2.2809 - val_accuracy: 0.4958\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 1s 83ms/step - loss: 0.4133 - accuracy: 0.8195 - val_loss: 2.1776 - val_accuracy: 0.5000\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 1s 80ms/step - loss: 0.4182 - accuracy: 0.8100 - val_loss: 2.4653 - val_accuracy: 0.4915\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 1s 82ms/step - loss: 0.4273 - accuracy: 0.8163 - val_loss: 2.5003 - val_accuracy: 0.5042\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 1s 81ms/step - loss: 0.4038 - accuracy: 0.8312 - val_loss: 2.4830 - val_accuracy: 0.4873\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 1s 80ms/step - loss: 0.4063 - accuracy: 0.8153 - val_loss: 2.8516 - val_accuracy: 0.5169\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 1s 81ms/step - loss: 0.3934 - accuracy: 0.8312 - val_loss: 2.4904 - val_accuracy: 0.5000\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 1s 81ms/step - loss: 0.3867 - accuracy: 0.8227 - val_loss: 2.5772 - val_accuracy: 0.4873\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 1s 82ms/step - loss: 0.3915 - accuracy: 0.8270 - val_loss: 2.7741 - val_accuracy: 0.4873\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 1s 80ms/step - loss: 0.3634 - accuracy: 0.8344 - val_loss: 2.5595 - val_accuracy: 0.5042\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 1s 84ms/step - loss: 0.3621 - accuracy: 0.8418 - val_loss: 2.8132 - val_accuracy: 0.4915\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 1s 81ms/step - loss: 0.3531 - accuracy: 0.8503 - val_loss: 2.7492 - val_accuracy: 0.5042\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 1s 82ms/step - loss: 0.3901 - accuracy: 0.8227 - val_loss: 2.8319 - val_accuracy: 0.5254\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 1s 80ms/step - loss: 0.3681 - accuracy: 0.8461 - val_loss: 2.9567 - val_accuracy: 0.5169\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 1s 82ms/step - loss: 0.3609 - accuracy: 0.8270 - val_loss: 2.6362 - val_accuracy: 0.5085\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 1s 81ms/step - loss: 0.3585 - accuracy: 0.8514 - val_loss: 2.7194 - val_accuracy: 0.5000\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 1s 79ms/step - loss: 0.5117 - accuracy: 0.7930 - val_loss: 2.6903 - val_accuracy: 0.4915\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 1s 82ms/step - loss: 0.4366 - accuracy: 0.8185 - val_loss: 2.3796 - val_accuracy: 0.5000\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 1s 80ms/step - loss: 0.3818 - accuracy: 0.8217 - val_loss: 2.7023 - val_accuracy: 0.4873\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 1s 80ms/step - loss: 0.3389 - accuracy: 0.8588 - val_loss: 2.6197 - val_accuracy: 0.4788\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 1s 79ms/step - loss: 0.3569 - accuracy: 0.8503 - val_loss: 3.0325 - val_accuracy: 0.4873\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 1s 80ms/step - loss: 0.3253 - accuracy: 0.8567 - val_loss: 2.7019 - val_accuracy: 0.4958\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 1s 81ms/step - loss: 0.3056 - accuracy: 0.8662 - val_loss: 3.0876 - val_accuracy: 0.4831\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 1s 79ms/step - loss: 0.3317 - accuracy: 0.8599 - val_loss: 3.0080 - val_accuracy: 0.4958\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 1s 79ms/step - loss: 0.3233 - accuracy: 0.8461 - val_loss: 3.0526 - val_accuracy: 0.5169\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 1s 82ms/step - loss: 0.3025 - accuracy: 0.8694 - val_loss: 3.0185 - val_accuracy: 0.4873\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 1s 82ms/step - loss: 0.2838 - accuracy: 0.8577 - val_loss: 3.0283 - val_accuracy: 0.4873\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 1s 80ms/step - loss: 0.2778 - accuracy: 0.8779 - val_loss: 3.2660 - val_accuracy: 0.4958\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 1s 81ms/step - loss: 0.2668 - accuracy: 0.8822 - val_loss: 3.1407 - val_accuracy: 0.4661\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 1s 80ms/step - loss: 0.2692 - accuracy: 0.8864 - val_loss: 3.2203 - val_accuracy: 0.4661\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 1s 80ms/step - loss: 0.2872 - accuracy: 0.8641 - val_loss: 3.2157 - val_accuracy: 0.5000\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 1s 81ms/step - loss: 0.2939 - accuracy: 0.8673 - val_loss: 3.3008 - val_accuracy: 0.5085\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 1s 81ms/step - loss: 0.3041 - accuracy: 0.8609 - val_loss: 3.4112 - val_accuracy: 0.5000\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 1s 81ms/step - loss: 0.3015 - accuracy: 0.8694 - val_loss: 3.1129 - val_accuracy: 0.4831\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOiwxlnsMHjF",
        "outputId": "4ef64004-c7eb-41a7-bcac-9f1a9e2ac7ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "model1.evaluate(X_test2,y_test2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 0s 8ms/step - loss: 2.9320 - accuracy: 0.4407\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.9320480823516846, 0.4406779706478119]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOic9WtEaRcT"
      },
      "source": [
        "**3rd Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYDszBuvMSOd"
      },
      "source": [
        "# 3rd Dataset\n",
        "names1=['Train or Test','Speaker Number','Sex','Feature 0','Feature 1','Feature 2','Feature 3','Feature 4','Feature 5','Feature 6','Feature 7','Feature 8','Feature 9','Feature 10','Class']\n",
        "df4=pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/vowel/vowel-context.data',names=names1,sep=' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH-yWJvFrRRT",
        "outputId": "4083a0cc-94cb-4b7f-99e1-d2c83fc8a267",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "df4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Train or Test</th>\n",
              "      <th>Speaker Number</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Feature 0</th>\n",
              "      <th>Feature 1</th>\n",
              "      <th>Feature 2</th>\n",
              "      <th>Feature 3</th>\n",
              "      <th>Feature 4</th>\n",
              "      <th>Feature 5</th>\n",
              "      <th>Feature 6</th>\n",
              "      <th>Feature 7</th>\n",
              "      <th>Feature 8</th>\n",
              "      <th>Feature 9</th>\n",
              "      <th>Feature 10</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-3.639</td>\n",
              "      <td>0.418</td>\n",
              "      <td>-0.670</td>\n",
              "      <td>1.779</td>\n",
              "      <td>-0.168</td>\n",
              "      <td>1.627</td>\n",
              "      <td>-0.388</td>\n",
              "      <td>0.529</td>\n",
              "      <td>-0.874</td>\n",
              "      <td>-0.814</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-3.327</td>\n",
              "      <td>0.496</td>\n",
              "      <td>-0.694</td>\n",
              "      <td>1.365</td>\n",
              "      <td>-0.265</td>\n",
              "      <td>1.933</td>\n",
              "      <td>-0.363</td>\n",
              "      <td>0.510</td>\n",
              "      <td>-0.621</td>\n",
              "      <td>-0.488</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-2.120</td>\n",
              "      <td>0.894</td>\n",
              "      <td>-1.576</td>\n",
              "      <td>0.147</td>\n",
              "      <td>-0.707</td>\n",
              "      <td>1.559</td>\n",
              "      <td>-0.579</td>\n",
              "      <td>0.676</td>\n",
              "      <td>-0.809</td>\n",
              "      <td>-0.049</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-2.287</td>\n",
              "      <td>1.809</td>\n",
              "      <td>-1.498</td>\n",
              "      <td>1.012</td>\n",
              "      <td>-1.053</td>\n",
              "      <td>1.060</td>\n",
              "      <td>-0.567</td>\n",
              "      <td>0.235</td>\n",
              "      <td>-0.091</td>\n",
              "      <td>-0.795</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-2.598</td>\n",
              "      <td>1.938</td>\n",
              "      <td>-0.846</td>\n",
              "      <td>1.062</td>\n",
              "      <td>-1.633</td>\n",
              "      <td>0.764</td>\n",
              "      <td>0.394</td>\n",
              "      <td>-0.150</td>\n",
              "      <td>0.277</td>\n",
              "      <td>-0.396</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>985</th>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>-3.239</td>\n",
              "      <td>3.083</td>\n",
              "      <td>-1.427</td>\n",
              "      <td>-0.202</td>\n",
              "      <td>-0.282</td>\n",
              "      <td>1.421</td>\n",
              "      <td>0.576</td>\n",
              "      <td>0.068</td>\n",
              "      <td>-0.914</td>\n",
              "      <td>0.147</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>986</th>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>-3.753</td>\n",
              "      <td>3.605</td>\n",
              "      <td>-0.899</td>\n",
              "      <td>-0.747</td>\n",
              "      <td>-0.401</td>\n",
              "      <td>1.765</td>\n",
              "      <td>0.620</td>\n",
              "      <td>0.754</td>\n",
              "      <td>-0.835</td>\n",
              "      <td>-0.301</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987</th>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>-3.980</td>\n",
              "      <td>2.459</td>\n",
              "      <td>0.068</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.237</td>\n",
              "      <td>1.029</td>\n",
              "      <td>-0.189</td>\n",
              "      <td>0.521</td>\n",
              "      <td>-0.773</td>\n",
              "      <td>-0.500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>988</th>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>-4.264</td>\n",
              "      <td>2.925</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.794</td>\n",
              "      <td>0.323</td>\n",
              "      <td>0.515</td>\n",
              "      <td>-1.282</td>\n",
              "      <td>-0.140</td>\n",
              "      <td>-0.863</td>\n",
              "      <td>-0.390</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>989</th>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>-3.291</td>\n",
              "      <td>2.324</td>\n",
              "      <td>-0.679</td>\n",
              "      <td>0.285</td>\n",
              "      <td>0.441</td>\n",
              "      <td>0.557</td>\n",
              "      <td>-0.227</td>\n",
              "      <td>0.115</td>\n",
              "      <td>-1.046</td>\n",
              "      <td>0.697</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>990 rows Ã— 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Train or Test  Speaker Number  Sex  ...  Feature 9  Feature 10  Class\n",
              "0                0               0    0  ...     -0.814         NaN    0.0\n",
              "1                0               0    0  ...     -0.488         NaN    1.0\n",
              "2                0               0    0  ...     -0.049         NaN    2.0\n",
              "3                0               0    0  ...     -0.795         NaN    3.0\n",
              "4                0               0    0  ...     -0.396         NaN    4.0\n",
              "..             ...             ...  ...  ...        ...         ...    ...\n",
              "985              1              14    1  ...      0.147         NaN    6.0\n",
              "986              1              14    1  ...     -0.301         NaN    7.0\n",
              "987              1              14    1  ...     -0.500         NaN    8.0\n",
              "988              1              14    1  ...     -0.390         NaN    9.0\n",
              "989              1              14    1  ...      0.697         NaN   10.0\n",
              "\n",
              "[990 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSUcxfnzrTGK"
      },
      "source": [
        "df4.drop(columns={'Feature 10'},inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rlm_muSj_ahB",
        "outputId": "b631304b-e6e5-49fc-c74e-2b24b5de5a32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "df4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Train or Test</th>\n",
              "      <th>Speaker Number</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Feature 0</th>\n",
              "      <th>Feature 1</th>\n",
              "      <th>Feature 2</th>\n",
              "      <th>Feature 3</th>\n",
              "      <th>Feature 4</th>\n",
              "      <th>Feature 5</th>\n",
              "      <th>Feature 6</th>\n",
              "      <th>Feature 7</th>\n",
              "      <th>Feature 8</th>\n",
              "      <th>Feature 9</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-3.639</td>\n",
              "      <td>0.418</td>\n",
              "      <td>-0.670</td>\n",
              "      <td>1.779</td>\n",
              "      <td>-0.168</td>\n",
              "      <td>1.627</td>\n",
              "      <td>-0.388</td>\n",
              "      <td>0.529</td>\n",
              "      <td>-0.874</td>\n",
              "      <td>-0.814</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-3.327</td>\n",
              "      <td>0.496</td>\n",
              "      <td>-0.694</td>\n",
              "      <td>1.365</td>\n",
              "      <td>-0.265</td>\n",
              "      <td>1.933</td>\n",
              "      <td>-0.363</td>\n",
              "      <td>0.510</td>\n",
              "      <td>-0.621</td>\n",
              "      <td>-0.488</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-2.120</td>\n",
              "      <td>0.894</td>\n",
              "      <td>-1.576</td>\n",
              "      <td>0.147</td>\n",
              "      <td>-0.707</td>\n",
              "      <td>1.559</td>\n",
              "      <td>-0.579</td>\n",
              "      <td>0.676</td>\n",
              "      <td>-0.809</td>\n",
              "      <td>-0.049</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-2.287</td>\n",
              "      <td>1.809</td>\n",
              "      <td>-1.498</td>\n",
              "      <td>1.012</td>\n",
              "      <td>-1.053</td>\n",
              "      <td>1.060</td>\n",
              "      <td>-0.567</td>\n",
              "      <td>0.235</td>\n",
              "      <td>-0.091</td>\n",
              "      <td>-0.795</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-2.598</td>\n",
              "      <td>1.938</td>\n",
              "      <td>-0.846</td>\n",
              "      <td>1.062</td>\n",
              "      <td>-1.633</td>\n",
              "      <td>0.764</td>\n",
              "      <td>0.394</td>\n",
              "      <td>-0.150</td>\n",
              "      <td>0.277</td>\n",
              "      <td>-0.396</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>985</th>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>-3.239</td>\n",
              "      <td>3.083</td>\n",
              "      <td>-1.427</td>\n",
              "      <td>-0.202</td>\n",
              "      <td>-0.282</td>\n",
              "      <td>1.421</td>\n",
              "      <td>0.576</td>\n",
              "      <td>0.068</td>\n",
              "      <td>-0.914</td>\n",
              "      <td>0.147</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>986</th>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>-3.753</td>\n",
              "      <td>3.605</td>\n",
              "      <td>-0.899</td>\n",
              "      <td>-0.747</td>\n",
              "      <td>-0.401</td>\n",
              "      <td>1.765</td>\n",
              "      <td>0.620</td>\n",
              "      <td>0.754</td>\n",
              "      <td>-0.835</td>\n",
              "      <td>-0.301</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987</th>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>-3.980</td>\n",
              "      <td>2.459</td>\n",
              "      <td>0.068</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.237</td>\n",
              "      <td>1.029</td>\n",
              "      <td>-0.189</td>\n",
              "      <td>0.521</td>\n",
              "      <td>-0.773</td>\n",
              "      <td>-0.500</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>988</th>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>-4.264</td>\n",
              "      <td>2.925</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.794</td>\n",
              "      <td>0.323</td>\n",
              "      <td>0.515</td>\n",
              "      <td>-1.282</td>\n",
              "      <td>-0.140</td>\n",
              "      <td>-0.863</td>\n",
              "      <td>-0.390</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>989</th>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>-3.291</td>\n",
              "      <td>2.324</td>\n",
              "      <td>-0.679</td>\n",
              "      <td>0.285</td>\n",
              "      <td>0.441</td>\n",
              "      <td>0.557</td>\n",
              "      <td>-0.227</td>\n",
              "      <td>0.115</td>\n",
              "      <td>-1.046</td>\n",
              "      <td>0.697</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>990 rows Ã— 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Train or Test  Speaker Number  Sex  ...  Feature 8  Feature 9  Class\n",
              "0                0               0    0  ...     -0.874     -0.814    0.0\n",
              "1                0               0    0  ...     -0.621     -0.488    1.0\n",
              "2                0               0    0  ...     -0.809     -0.049    2.0\n",
              "3                0               0    0  ...     -0.091     -0.795    3.0\n",
              "4                0               0    0  ...      0.277     -0.396    4.0\n",
              "..             ...             ...  ...  ...        ...        ...    ...\n",
              "985              1              14    1  ...     -0.914      0.147    6.0\n",
              "986              1              14    1  ...     -0.835     -0.301    7.0\n",
              "987              1              14    1  ...     -0.773     -0.500    8.0\n",
              "988              1              14    1  ...     -0.863     -0.390    9.0\n",
              "989              1              14    1  ...     -1.046      0.697   10.0\n",
              "\n",
              "[990 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxlMCoHZ_beN"
      },
      "source": [
        "# Dividing into train and test\n",
        "train=df4[df4['Train or Test']==0]\n",
        "test=df4[df4['Train or Test']==1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2ml7N9KGUnv",
        "outputId": "fe91154e-fbff-493a-cdac-80ae11dbb972",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "# Dropping the 'Train or Test' and 'Speaker Number' Column as it is not of much importance for analysis\n",
        "y_train=train['Class']\n",
        "y_test=test['Class']\n",
        "train.drop(columns={'Train or Test','Speaker Number','Class'},inplace=True)\n",
        "test.drop(columns={'Train or Test','Speaker Number','Class'},inplace=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsI6EqW4_k2Q",
        "outputId": "0bdc274c-5f9f-4599-8b43-1bf5620290c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sex</th>\n",
              "      <th>Feature 0</th>\n",
              "      <th>Feature 1</th>\n",
              "      <th>Feature 2</th>\n",
              "      <th>Feature 3</th>\n",
              "      <th>Feature 4</th>\n",
              "      <th>Feature 5</th>\n",
              "      <th>Feature 6</th>\n",
              "      <th>Feature 7</th>\n",
              "      <th>Feature 8</th>\n",
              "      <th>Feature 9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-3.639</td>\n",
              "      <td>0.418</td>\n",
              "      <td>-0.670</td>\n",
              "      <td>1.779</td>\n",
              "      <td>-0.168</td>\n",
              "      <td>1.627</td>\n",
              "      <td>-0.388</td>\n",
              "      <td>0.529</td>\n",
              "      <td>-0.874</td>\n",
              "      <td>-0.814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>-3.327</td>\n",
              "      <td>0.496</td>\n",
              "      <td>-0.694</td>\n",
              "      <td>1.365</td>\n",
              "      <td>-0.265</td>\n",
              "      <td>1.933</td>\n",
              "      <td>-0.363</td>\n",
              "      <td>0.510</td>\n",
              "      <td>-0.621</td>\n",
              "      <td>-0.488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>-2.120</td>\n",
              "      <td>0.894</td>\n",
              "      <td>-1.576</td>\n",
              "      <td>0.147</td>\n",
              "      <td>-0.707</td>\n",
              "      <td>1.559</td>\n",
              "      <td>-0.579</td>\n",
              "      <td>0.676</td>\n",
              "      <td>-0.809</td>\n",
              "      <td>-0.049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>-2.287</td>\n",
              "      <td>1.809</td>\n",
              "      <td>-1.498</td>\n",
              "      <td>1.012</td>\n",
              "      <td>-1.053</td>\n",
              "      <td>1.060</td>\n",
              "      <td>-0.567</td>\n",
              "      <td>0.235</td>\n",
              "      <td>-0.091</td>\n",
              "      <td>-0.795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>-2.598</td>\n",
              "      <td>1.938</td>\n",
              "      <td>-0.846</td>\n",
              "      <td>1.062</td>\n",
              "      <td>-1.633</td>\n",
              "      <td>0.764</td>\n",
              "      <td>0.394</td>\n",
              "      <td>-0.150</td>\n",
              "      <td>0.277</td>\n",
              "      <td>-0.396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>523</th>\n",
              "      <td>1</td>\n",
              "      <td>-4.065</td>\n",
              "      <td>2.876</td>\n",
              "      <td>-0.856</td>\n",
              "      <td>-0.221</td>\n",
              "      <td>-0.533</td>\n",
              "      <td>0.232</td>\n",
              "      <td>0.855</td>\n",
              "      <td>0.633</td>\n",
              "      <td>-1.452</td>\n",
              "      <td>0.272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>524</th>\n",
              "      <td>1</td>\n",
              "      <td>-4.513</td>\n",
              "      <td>4.265</td>\n",
              "      <td>-1.477</td>\n",
              "      <td>-1.090</td>\n",
              "      <td>0.215</td>\n",
              "      <td>0.829</td>\n",
              "      <td>0.342</td>\n",
              "      <td>0.693</td>\n",
              "      <td>-0.601</td>\n",
              "      <td>-0.056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>525</th>\n",
              "      <td>1</td>\n",
              "      <td>-4.651</td>\n",
              "      <td>4.246</td>\n",
              "      <td>-0.823</td>\n",
              "      <td>-0.831</td>\n",
              "      <td>0.666</td>\n",
              "      <td>0.546</td>\n",
              "      <td>-0.300</td>\n",
              "      <td>0.094</td>\n",
              "      <td>-1.343</td>\n",
              "      <td>0.185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>526</th>\n",
              "      <td>1</td>\n",
              "      <td>-5.034</td>\n",
              "      <td>4.993</td>\n",
              "      <td>-1.633</td>\n",
              "      <td>-0.285</td>\n",
              "      <td>0.398</td>\n",
              "      <td>0.181</td>\n",
              "      <td>-0.211</td>\n",
              "      <td>-0.508</td>\n",
              "      <td>-0.283</td>\n",
              "      <td>0.304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>527</th>\n",
              "      <td>1</td>\n",
              "      <td>-4.261</td>\n",
              "      <td>1.827</td>\n",
              "      <td>-0.482</td>\n",
              "      <td>-0.194</td>\n",
              "      <td>0.731</td>\n",
              "      <td>0.354</td>\n",
              "      <td>-0.478</td>\n",
              "      <td>0.050</td>\n",
              "      <td>-0.112</td>\n",
              "      <td>0.321</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>528 rows Ã— 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Sex  Feature 0  Feature 1  ...  Feature 7  Feature 8  Feature 9\n",
              "0      0     -3.639      0.418  ...      0.529     -0.874     -0.814\n",
              "1      0     -3.327      0.496  ...      0.510     -0.621     -0.488\n",
              "2      0     -2.120      0.894  ...      0.676     -0.809     -0.049\n",
              "3      0     -2.287      1.809  ...      0.235     -0.091     -0.795\n",
              "4      0     -2.598      1.938  ...     -0.150      0.277     -0.396\n",
              "..   ...        ...        ...  ...        ...        ...        ...\n",
              "523    1     -4.065      2.876  ...      0.633     -1.452      0.272\n",
              "524    1     -4.513      4.265  ...      0.693     -0.601     -0.056\n",
              "525    1     -4.651      4.246  ...      0.094     -1.343      0.185\n",
              "526    1     -5.034      4.993  ...     -0.508     -0.283      0.304\n",
              "527    1     -4.261      1.827  ...      0.050     -0.112      0.321\n",
              "\n",
              "[528 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiVYeKC__qLT",
        "outputId": "61416803-ef9c-4a47-8f44-ce720efe29aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sex</th>\n",
              "      <th>Feature 0</th>\n",
              "      <th>Feature 1</th>\n",
              "      <th>Feature 2</th>\n",
              "      <th>Feature 3</th>\n",
              "      <th>Feature 4</th>\n",
              "      <th>Feature 5</th>\n",
              "      <th>Feature 6</th>\n",
              "      <th>Feature 7</th>\n",
              "      <th>Feature 8</th>\n",
              "      <th>Feature 9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>528</th>\n",
              "      <td>0</td>\n",
              "      <td>-1.149</td>\n",
              "      <td>-0.904</td>\n",
              "      <td>-1.988</td>\n",
              "      <td>0.739</td>\n",
              "      <td>-0.060</td>\n",
              "      <td>1.206</td>\n",
              "      <td>0.864</td>\n",
              "      <td>1.196</td>\n",
              "      <td>-0.300</td>\n",
              "      <td>-0.467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>529</th>\n",
              "      <td>0</td>\n",
              "      <td>-2.613</td>\n",
              "      <td>-0.092</td>\n",
              "      <td>-0.540</td>\n",
              "      <td>0.484</td>\n",
              "      <td>0.389</td>\n",
              "      <td>1.741</td>\n",
              "      <td>0.198</td>\n",
              "      <td>0.257</td>\n",
              "      <td>-0.375</td>\n",
              "      <td>-0.604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>530</th>\n",
              "      <td>0</td>\n",
              "      <td>-2.505</td>\n",
              "      <td>0.632</td>\n",
              "      <td>-0.593</td>\n",
              "      <td>0.304</td>\n",
              "      <td>0.496</td>\n",
              "      <td>0.824</td>\n",
              "      <td>-0.162</td>\n",
              "      <td>0.181</td>\n",
              "      <td>-0.363</td>\n",
              "      <td>-0.764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>531</th>\n",
              "      <td>0</td>\n",
              "      <td>-1.768</td>\n",
              "      <td>1.769</td>\n",
              "      <td>-1.142</td>\n",
              "      <td>-0.739</td>\n",
              "      <td>-0.086</td>\n",
              "      <td>0.120</td>\n",
              "      <td>-0.230</td>\n",
              "      <td>0.217</td>\n",
              "      <td>-0.009</td>\n",
              "      <td>-0.279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>532</th>\n",
              "      <td>0</td>\n",
              "      <td>-2.671</td>\n",
              "      <td>3.155</td>\n",
              "      <td>-0.514</td>\n",
              "      <td>0.133</td>\n",
              "      <td>-0.964</td>\n",
              "      <td>0.234</td>\n",
              "      <td>-0.071</td>\n",
              "      <td>1.192</td>\n",
              "      <td>0.254</td>\n",
              "      <td>-0.471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>985</th>\n",
              "      <td>1</td>\n",
              "      <td>-3.239</td>\n",
              "      <td>3.083</td>\n",
              "      <td>-1.427</td>\n",
              "      <td>-0.202</td>\n",
              "      <td>-0.282</td>\n",
              "      <td>1.421</td>\n",
              "      <td>0.576</td>\n",
              "      <td>0.068</td>\n",
              "      <td>-0.914</td>\n",
              "      <td>0.147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>986</th>\n",
              "      <td>1</td>\n",
              "      <td>-3.753</td>\n",
              "      <td>3.605</td>\n",
              "      <td>-0.899</td>\n",
              "      <td>-0.747</td>\n",
              "      <td>-0.401</td>\n",
              "      <td>1.765</td>\n",
              "      <td>0.620</td>\n",
              "      <td>0.754</td>\n",
              "      <td>-0.835</td>\n",
              "      <td>-0.301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987</th>\n",
              "      <td>1</td>\n",
              "      <td>-3.980</td>\n",
              "      <td>2.459</td>\n",
              "      <td>0.068</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.237</td>\n",
              "      <td>1.029</td>\n",
              "      <td>-0.189</td>\n",
              "      <td>0.521</td>\n",
              "      <td>-0.773</td>\n",
              "      <td>-0.500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>988</th>\n",
              "      <td>1</td>\n",
              "      <td>-4.264</td>\n",
              "      <td>2.925</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.794</td>\n",
              "      <td>0.323</td>\n",
              "      <td>0.515</td>\n",
              "      <td>-1.282</td>\n",
              "      <td>-0.140</td>\n",
              "      <td>-0.863</td>\n",
              "      <td>-0.390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>989</th>\n",
              "      <td>1</td>\n",
              "      <td>-3.291</td>\n",
              "      <td>2.324</td>\n",
              "      <td>-0.679</td>\n",
              "      <td>0.285</td>\n",
              "      <td>0.441</td>\n",
              "      <td>0.557</td>\n",
              "      <td>-0.227</td>\n",
              "      <td>0.115</td>\n",
              "      <td>-1.046</td>\n",
              "      <td>0.697</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>462 rows Ã— 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Sex  Feature 0  Feature 1  ...  Feature 7  Feature 8  Feature 9\n",
              "528    0     -1.149     -0.904  ...      1.196     -0.300     -0.467\n",
              "529    0     -2.613     -0.092  ...      0.257     -0.375     -0.604\n",
              "530    0     -2.505      0.632  ...      0.181     -0.363     -0.764\n",
              "531    0     -1.768      1.769  ...      0.217     -0.009     -0.279\n",
              "532    0     -2.671      3.155  ...      1.192      0.254     -0.471\n",
              "..   ...        ...        ...  ...        ...        ...        ...\n",
              "985    1     -3.239      3.083  ...      0.068     -0.914      0.147\n",
              "986    1     -3.753      3.605  ...      0.754     -0.835     -0.301\n",
              "987    1     -3.980      2.459  ...      0.521     -0.773     -0.500\n",
              "988    1     -4.264      2.925  ...     -0.140     -0.863     -0.390\n",
              "989    1     -3.291      2.324  ...      0.115     -1.046      0.697\n",
              "\n",
              "[462 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ciRlPZVGOBC",
        "outputId": "16754440-d144-4eae-bf0d-44adeb0c1fba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "y_train.replace(np.nan,10,inplace=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:6746: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._update_inplace(new_data)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcoPkuidIAZa",
        "outputId": "49999a9c-acdc-4557-bb7c-0622a9109c5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "y_test.replace(np.nan,10,inplace=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:6746: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._update_inplace(new_data)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eOmUT_lIrgZ",
        "outputId": "578d47b7-e40a-4b29-8ecc-cbaf4e4083f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "train.isna().sum()/len(train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sex          0.0\n",
              "Feature 0    0.0\n",
              "Feature 1    0.0\n",
              "Feature 2    0.0\n",
              "Feature 3    0.0\n",
              "Feature 4    0.0\n",
              "Feature 5    0.0\n",
              "Feature 6    0.0\n",
              "Feature 7    0.0\n",
              "Feature 8    0.0\n",
              "Feature 9    0.0\n",
              "Class        0.0\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaLaKHMrIvpb",
        "outputId": "3ba3f741-bb3e-4b8c-f2c1-63c50722be29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "test.isna().sum()/len(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sex          0.0\n",
              "Feature 0    0.0\n",
              "Feature 1    0.0\n",
              "Feature 2    0.0\n",
              "Feature 3    0.0\n",
              "Feature 4    0.0\n",
              "Feature 5    0.0\n",
              "Feature 6    0.0\n",
              "Feature 7    0.0\n",
              "Feature 8    0.0\n",
              "Feature 9    0.0\n",
              "Class        0.0\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnYxwgfZJHHt",
        "outputId": "b008fd56-b569-4847-88ea-b244c091ae57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "# Checking the distribution of the target class\n",
        "import seaborn as sns\n",
        "sns.countplot(y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fa4a5c8d208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeBbLhXCJHMi"
      },
      "source": [
        "from sklearn import linear_model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logistic=linear_model.LogisticRegression()\n",
        "sgd=linear_model.SGDClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUPDTg-2ShpL",
        "outputId": "4d83ff09-ea1f-4596-8af5-442b043399b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        }
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "scores=cross_val_score(logistic,train,y_train,cv=5,scoring='accuracy')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPH-x60dShsg",
        "outputId": "56dc04cd-ff2f-4a63-c119-cd30e40e5933",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.mean(scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6800359389038634"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdBR9D5ySh1n"
      },
      "source": [
        "scores_sgd=cross_val_score(sgd,train,y_train,cv=5,scoring='accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLarZpA0Sh0I",
        "outputId": "5def1540-1b63-4d3f-e3fa-f62da0c493b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.mean(scores_sgd)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4586163522012579"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ8DtByKShym"
      },
      "source": [
        "# GridSearch CV on Logistic Regression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "gscv_log=GridSearchCV(logistic, {\n",
        "                      'penalty': ['l1','l2','elasticnet'],\n",
        "                      'C': [0.1,0.5,0.25,0.8,0.9,0.95,1.5,2.3,5.5],\n",
        "                      'class_weight' : [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}],\n",
        "                      'solver' : ['liblinear', 'saga']\n",
        "                      },cv=5,return_train_score=False,n_jobs=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIOo09i6Shw-",
        "outputId": "c79eea57-dd54-47b7-8fdf-7d8daf87a417",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "source": [
        "gscv_log.fit(train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                          fit_intercept=True,\n",
              "                                          intercept_scaling=1, l1_ratio=None,\n",
              "                                          max_iter=100, multi_class='auto',\n",
              "                                          n_jobs=None, penalty='l2',\n",
              "                                          random_state=None, solver='lbfgs',\n",
              "                                          tol=0.0001, verbose=0,\n",
              "                                          warm_start=False),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid={'C': [0.1, 0.5, 0.25, 0.8, 0.9, 0.95, 1.5, 2.3, 5.5],\n",
              "                         'class_weight': [{0: 0.5, 1: 0.5}, {0: 0.6, 1: 0.4}],\n",
              "                         'penalty': ['l1', 'l2', 'elasticnet'],\n",
              "                         'solver': ['liblinear', 'saga']},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDdue7-RShmy",
        "outputId": "b47012cc-0ac2-4afb-f80c-e5705d4376ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "gscv_log.best_score_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6721832884097034"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4dh9ParXKI4",
        "outputId": "57ec12d8-f46f-4d60-d721-93b496facf86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Accuracy on the Test Set\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_test_pred = gscv_log.predict(test)\n",
        "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
        "print(\"Accuracy:\"+str(accuracy_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:0.7359307359307359\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgZ27N-BXYzj",
        "outputId": "522c9d0f-fcf9-44aa-de5a-5206a06cf6b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "# Performing Random Forest on the Dataset\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf=RandomForestClassifier()\n",
        "rf.fit(train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7drv-MqXifV",
        "outputId": "eeb6fc0b-cfc2-42f1-9dda-f9b6567c514a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_test_pred_rf = rf.predict(test)\n",
        "accuracy_test = accuracy_score(y_test, y_test_pred_rf)\n",
        "print(\"Accuracy:\"+str(accuracy_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:0.9826839826839827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPnVv9L-Xx5O"
      },
      "source": [
        "# Performing Neural Networks on the Dataset\n",
        "# Importing the Necessary Libraries for Neural Networks\n",
        "from keras import Sequential\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Dense\n",
        "import tensorflow\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oPhoL6aYDkk"
      },
      "source": [
        "y_train=to_categorical(y_train)\n",
        "y_test=to_categorical(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmGHYEFPYmoE",
        "outputId": "c93d41d0-def2-46af-ffff-fb83bea72cdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(528, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAQjxbylYoLR",
        "outputId": "f8074d01-ec38-4151-c8cf-a22f0b3d2925",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(528, 11)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX2FAXVFYc8R",
        "outputId": "18568af8-ada4-486e-b272-b9a500114892",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Performing Neural Network on train and y_train\n",
        "model=Sequential()\n",
        "model.add(Dense(units=58, activation='relu', input_dim=12))\n",
        "model.add(Dense(units=280, activation='relu'))\n",
        "model.add(Dense(units=101, activation='relu'))\n",
        "model.add(Dense(units=21))\n",
        "model.add(LeakyReLU(alpha=0.05))\n",
        "model.add(Dense(units=11, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "history1=model.fit(train, y_train, epochs=200,validation_split=0.2, verbose=1,batch_size=128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 2.3675 - accuracy: 0.1635 - val_loss: 2.1354 - val_accuracy: 0.1792\n",
            "Epoch 2/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 2.0107 - accuracy: 0.2749 - val_loss: 1.9649 - val_accuracy: 0.2170\n",
            "Epoch 3/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.8166 - accuracy: 0.3175 - val_loss: 1.9921 - val_accuracy: 0.2547\n",
            "Epoch 4/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.7330 - accuracy: 0.3246 - val_loss: 1.7632 - val_accuracy: 0.3396\n",
            "Epoch 5/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.5694 - accuracy: 0.4502 - val_loss: 1.7659 - val_accuracy: 0.2925\n",
            "Epoch 6/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.4597 - accuracy: 0.4905 - val_loss: 1.5797 - val_accuracy: 0.4717\n",
            "Epoch 7/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.3302 - accuracy: 0.5403 - val_loss: 1.6519 - val_accuracy: 0.3491\n",
            "Epoch 8/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.3947 - accuracy: 0.5071 - val_loss: 1.4547 - val_accuracy: 0.5472\n",
            "Epoch 9/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.2195 - accuracy: 0.5924 - val_loss: 1.4752 - val_accuracy: 0.4434\n",
            "Epoch 10/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.1653 - accuracy: 0.6019 - val_loss: 1.3403 - val_accuracy: 0.5094\n",
            "Epoch 11/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.0483 - accuracy: 0.6493 - val_loss: 1.4344 - val_accuracy: 0.4151\n",
            "Epoch 12/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9919 - accuracy: 0.6872 - val_loss: 1.3698 - val_accuracy: 0.4623\n",
            "Epoch 13/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9640 - accuracy: 0.7038 - val_loss: 1.6023 - val_accuracy: 0.4528\n",
            "Epoch 14/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9354 - accuracy: 0.6754 - val_loss: 1.4390 - val_accuracy: 0.4623\n",
            "Epoch 15/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8846 - accuracy: 0.7062 - val_loss: 1.5430 - val_accuracy: 0.4623\n",
            "Epoch 16/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.8992 - accuracy: 0.6991 - val_loss: 1.4030 - val_accuracy: 0.4717\n",
            "Epoch 17/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7345 - accuracy: 0.7749 - val_loss: 1.4310 - val_accuracy: 0.4434\n",
            "Epoch 18/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7446 - accuracy: 0.7583 - val_loss: 1.4084 - val_accuracy: 0.4245\n",
            "Epoch 19/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7722 - accuracy: 0.7536 - val_loss: 1.6011 - val_accuracy: 0.4717\n",
            "Epoch 20/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7378 - accuracy: 0.7417 - val_loss: 1.5597 - val_accuracy: 0.4623\n",
            "Epoch 21/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6590 - accuracy: 0.7867 - val_loss: 1.3023 - val_accuracy: 0.5283\n",
            "Epoch 22/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6250 - accuracy: 0.8009 - val_loss: 1.4975 - val_accuracy: 0.5189\n",
            "Epoch 23/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6145 - accuracy: 0.7844 - val_loss: 1.1969 - val_accuracy: 0.5566\n",
            "Epoch 24/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5624 - accuracy: 0.8294 - val_loss: 2.1822 - val_accuracy: 0.4057\n",
            "Epoch 25/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6657 - accuracy: 0.7156 - val_loss: 1.7222 - val_accuracy: 0.4434\n",
            "Epoch 26/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6436 - accuracy: 0.7417 - val_loss: 1.3297 - val_accuracy: 0.4528\n",
            "Epoch 27/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5314 - accuracy: 0.7986 - val_loss: 1.4054 - val_accuracy: 0.5283\n",
            "Epoch 28/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5540 - accuracy: 0.8081 - val_loss: 1.5073 - val_accuracy: 0.5000\n",
            "Epoch 29/200\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.4619 - accuracy: 0.8697 - val_loss: 2.1702 - val_accuracy: 0.4340\n",
            "Epoch 30/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4909 - accuracy: 0.8246 - val_loss: 1.9063 - val_accuracy: 0.5000\n",
            "Epoch 31/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5430 - accuracy: 0.7891 - val_loss: 1.4513 - val_accuracy: 0.4623\n",
            "Epoch 32/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4142 - accuracy: 0.8626 - val_loss: 1.6243 - val_accuracy: 0.4906\n",
            "Epoch 33/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5114 - accuracy: 0.8057 - val_loss: 1.3599 - val_accuracy: 0.4717\n",
            "Epoch 34/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4380 - accuracy: 0.8483 - val_loss: 1.6205 - val_accuracy: 0.4528\n",
            "Epoch 35/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4422 - accuracy: 0.8389 - val_loss: 1.2358 - val_accuracy: 0.4717\n",
            "Epoch 36/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4544 - accuracy: 0.8152 - val_loss: 1.6130 - val_accuracy: 0.4528\n",
            "Epoch 37/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.3529 - accuracy: 0.8910 - val_loss: 2.1677 - val_accuracy: 0.4151\n",
            "Epoch 38/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.3714 - accuracy: 0.8626 - val_loss: 1.6230 - val_accuracy: 0.5189\n",
            "Epoch 39/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5416 - accuracy: 0.7583 - val_loss: 1.7985 - val_accuracy: 0.4434\n",
            "Epoch 40/200\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.3734 - accuracy: 0.8673 - val_loss: 1.8174 - val_accuracy: 0.5000\n",
            "Epoch 41/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.3337 - accuracy: 0.8910 - val_loss: 1.5791 - val_accuracy: 0.4811\n",
            "Epoch 42/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.3899 - accuracy: 0.8507 - val_loss: 2.1448 - val_accuracy: 0.4528\n",
            "Epoch 43/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.3613 - accuracy: 0.8483 - val_loss: 2.4148 - val_accuracy: 0.4811\n",
            "Epoch 44/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.3742 - accuracy: 0.8436 - val_loss: 2.1080 - val_accuracy: 0.4717\n",
            "Epoch 45/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.3246 - accuracy: 0.8815 - val_loss: 2.2654 - val_accuracy: 0.4717\n",
            "Epoch 46/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4062 - accuracy: 0.8365 - val_loss: 1.7289 - val_accuracy: 0.4811\n",
            "Epoch 47/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2755 - accuracy: 0.9313 - val_loss: 1.9103 - val_accuracy: 0.5094\n",
            "Epoch 48/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2838 - accuracy: 0.9147 - val_loss: 2.0674 - val_accuracy: 0.4528\n",
            "Epoch 49/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.3328 - accuracy: 0.8744 - val_loss: 1.6229 - val_accuracy: 0.4906\n",
            "Epoch 50/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.3288 - accuracy: 0.8697 - val_loss: 2.3574 - val_accuracy: 0.4340\n",
            "Epoch 51/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.3191 - accuracy: 0.8934 - val_loss: 2.1906 - val_accuracy: 0.4717\n",
            "Epoch 52/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2815 - accuracy: 0.9005 - val_loss: 1.6810 - val_accuracy: 0.5094\n",
            "Epoch 53/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2449 - accuracy: 0.9289 - val_loss: 1.9110 - val_accuracy: 0.5000\n",
            "Epoch 54/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2848 - accuracy: 0.8815 - val_loss: 1.3339 - val_accuracy: 0.5377\n",
            "Epoch 55/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2678 - accuracy: 0.9194 - val_loss: 1.6024 - val_accuracy: 0.4906\n",
            "Epoch 56/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2327 - accuracy: 0.9289 - val_loss: 1.9030 - val_accuracy: 0.5189\n",
            "Epoch 57/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3288 - accuracy: 0.8863 - val_loss: 2.2129 - val_accuracy: 0.5000\n",
            "Epoch 58/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2390 - accuracy: 0.9384 - val_loss: 1.9532 - val_accuracy: 0.5000\n",
            "Epoch 59/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2809 - accuracy: 0.8768 - val_loss: 1.9351 - val_accuracy: 0.4811\n",
            "Epoch 60/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2812 - accuracy: 0.8934 - val_loss: 1.7330 - val_accuracy: 0.5283\n",
            "Epoch 61/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1997 - accuracy: 0.9479 - val_loss: 1.8584 - val_accuracy: 0.5283\n",
            "Epoch 62/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1703 - accuracy: 0.9597 - val_loss: 1.8277 - val_accuracy: 0.5566\n",
            "Epoch 63/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2313 - accuracy: 0.9052 - val_loss: 2.5619 - val_accuracy: 0.4717\n",
            "Epoch 64/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.3035 - accuracy: 0.8863 - val_loss: 1.9012 - val_accuracy: 0.5189\n",
            "Epoch 65/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2348 - accuracy: 0.9171 - val_loss: 1.9208 - val_accuracy: 0.5566\n",
            "Epoch 66/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1645 - accuracy: 0.9668 - val_loss: 2.2414 - val_accuracy: 0.5189\n",
            "Epoch 67/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1838 - accuracy: 0.9502 - val_loss: 1.6727 - val_accuracy: 0.5000\n",
            "Epoch 68/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2409 - accuracy: 0.9100 - val_loss: 1.7489 - val_accuracy: 0.5189\n",
            "Epoch 69/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2164 - accuracy: 0.9076 - val_loss: 2.3389 - val_accuracy: 0.5094\n",
            "Epoch 70/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1513 - accuracy: 0.9716 - val_loss: 2.2471 - val_accuracy: 0.5189\n",
            "Epoch 71/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1835 - accuracy: 0.9408 - val_loss: 3.1087 - val_accuracy: 0.4717\n",
            "Epoch 72/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2923 - accuracy: 0.9005 - val_loss: 2.1078 - val_accuracy: 0.5094\n",
            "Epoch 73/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1818 - accuracy: 0.9336 - val_loss: 2.2638 - val_accuracy: 0.5189\n",
            "Epoch 74/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1485 - accuracy: 0.9550 - val_loss: 2.2897 - val_accuracy: 0.5094\n",
            "Epoch 75/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1864 - accuracy: 0.9218 - val_loss: 2.1723 - val_accuracy: 0.5094\n",
            "Epoch 76/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.3224 - accuracy: 0.8720 - val_loss: 2.2439 - val_accuracy: 0.5189\n",
            "Epoch 77/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1152 - accuracy: 0.9810 - val_loss: 1.6412 - val_accuracy: 0.5000\n",
            "Epoch 78/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1347 - accuracy: 0.9668 - val_loss: 2.1192 - val_accuracy: 0.5189\n",
            "Epoch 79/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1958 - accuracy: 0.9171 - val_loss: 1.9244 - val_accuracy: 0.5189\n",
            "Epoch 80/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2017 - accuracy: 0.9123 - val_loss: 2.0679 - val_accuracy: 0.5189\n",
            "Epoch 81/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1304 - accuracy: 0.9550 - val_loss: 1.8878 - val_accuracy: 0.5189\n",
            "Epoch 82/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1320 - accuracy: 0.9479 - val_loss: 1.7006 - val_accuracy: 0.5755\n",
            "Epoch 83/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1274 - accuracy: 0.9573 - val_loss: 2.7784 - val_accuracy: 0.5000\n",
            "Epoch 84/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2944 - accuracy: 0.8886 - val_loss: 2.3886 - val_accuracy: 0.5283\n",
            "Epoch 85/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1130 - accuracy: 0.9787 - val_loss: 2.4824 - val_accuracy: 0.4811\n",
            "Epoch 86/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1286 - accuracy: 0.9502 - val_loss: 2.7396 - val_accuracy: 0.4811\n",
            "Epoch 87/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.1418 - accuracy: 0.9502 - val_loss: 1.8858 - val_accuracy: 0.5377\n",
            "Epoch 88/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.1310 - accuracy: 0.9455 - val_loss: 1.9623 - val_accuracy: 0.5189\n",
            "Epoch 89/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1482 - accuracy: 0.9455 - val_loss: 2.7149 - val_accuracy: 0.4811\n",
            "Epoch 90/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1689 - accuracy: 0.9384 - val_loss: 2.0496 - val_accuracy: 0.5000\n",
            "Epoch 91/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0902 - accuracy: 0.9692 - val_loss: 2.1467 - val_accuracy: 0.5283\n",
            "Epoch 92/200\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.1156 - accuracy: 0.9597 - val_loss: 2.6039 - val_accuracy: 0.5094\n",
            "Epoch 93/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1450 - accuracy: 0.9431 - val_loss: 1.8686 - val_accuracy: 0.5283\n",
            "Epoch 94/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.1338 - accuracy: 0.9621 - val_loss: 2.2281 - val_accuracy: 0.5660\n",
            "Epoch 95/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1428 - accuracy: 0.9431 - val_loss: 2.1776 - val_accuracy: 0.5000\n",
            "Epoch 96/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0830 - accuracy: 0.9834 - val_loss: 2.5122 - val_accuracy: 0.4906\n",
            "Epoch 97/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0702 - accuracy: 0.9858 - val_loss: 2.3802 - val_accuracy: 0.5189\n",
            "Epoch 98/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1028 - accuracy: 0.9787 - val_loss: 3.1918 - val_accuracy: 0.4340\n",
            "Epoch 99/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1704 - accuracy: 0.9313 - val_loss: 2.5720 - val_accuracy: 0.4717\n",
            "Epoch 100/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0864 - accuracy: 0.9858 - val_loss: 2.6232 - val_accuracy: 0.4811\n",
            "Epoch 101/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1125 - accuracy: 0.9597 - val_loss: 2.4086 - val_accuracy: 0.5283\n",
            "Epoch 102/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1500 - accuracy: 0.9408 - val_loss: 2.5412 - val_accuracy: 0.4717\n",
            "Epoch 103/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1371 - accuracy: 0.9455 - val_loss: 2.3758 - val_accuracy: 0.5000\n",
            "Epoch 104/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0630 - accuracy: 0.9858 - val_loss: 2.0958 - val_accuracy: 0.5377\n",
            "Epoch 105/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0584 - accuracy: 0.9929 - val_loss: 2.4258 - val_accuracy: 0.4906\n",
            "Epoch 106/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0471 - accuracy: 0.9953 - val_loss: 2.2202 - val_accuracy: 0.5000\n",
            "Epoch 107/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1999 - accuracy: 0.9336 - val_loss: 2.5585 - val_accuracy: 0.5943\n",
            "Epoch 108/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1008 - accuracy: 0.9668 - val_loss: 2.3193 - val_accuracy: 0.4906\n",
            "Epoch 109/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2175 - accuracy: 0.9336 - val_loss: 2.6039 - val_accuracy: 0.5094\n",
            "Epoch 110/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0527 - accuracy: 0.9953 - val_loss: 2.5220 - val_accuracy: 0.5000\n",
            "Epoch 111/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0639 - accuracy: 0.9810 - val_loss: 2.9237 - val_accuracy: 0.4717\n",
            "Epoch 112/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0781 - accuracy: 0.9763 - val_loss: 2.4127 - val_accuracy: 0.5000\n",
            "Epoch 113/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0467 - accuracy: 0.9953 - val_loss: 2.5596 - val_accuracy: 0.5000\n",
            "Epoch 114/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0507 - accuracy: 0.9929 - val_loss: 2.7753 - val_accuracy: 0.5000\n",
            "Epoch 115/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0731 - accuracy: 0.9763 - val_loss: 3.0099 - val_accuracy: 0.5094\n",
            "Epoch 116/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0753 - accuracy: 0.9716 - val_loss: 2.0834 - val_accuracy: 0.5472\n",
            "Epoch 117/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0422 - accuracy: 0.9905 - val_loss: 2.4359 - val_accuracy: 0.5283\n",
            "Epoch 118/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0667 - accuracy: 0.9692 - val_loss: 2.2926 - val_accuracy: 0.5189\n",
            "Epoch 119/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0898 - accuracy: 0.9621 - val_loss: 2.6157 - val_accuracy: 0.5660\n",
            "Epoch 120/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1361 - accuracy: 0.9550 - val_loss: 2.5238 - val_accuracy: 0.5283\n",
            "Epoch 121/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0423 - accuracy: 0.9929 - val_loss: 2.5740 - val_accuracy: 0.4811\n",
            "Epoch 122/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0312 - accuracy: 1.0000 - val_loss: 2.7893 - val_accuracy: 0.5000\n",
            "Epoch 123/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0486 - accuracy: 0.9882 - val_loss: 3.5924 - val_accuracy: 0.4434\n",
            "Epoch 124/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.1575 - accuracy: 0.9526 - val_loss: 2.6250 - val_accuracy: 0.5000\n",
            "Epoch 125/200\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.0319 - accuracy: 1.0000 - val_loss: 2.6866 - val_accuracy: 0.5000\n",
            "Epoch 126/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0297 - accuracy: 0.9976 - val_loss: 2.7033 - val_accuracy: 0.5189\n",
            "Epoch 127/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1576 - accuracy: 0.9479 - val_loss: 2.5757 - val_accuracy: 0.5660\n",
            "Epoch 128/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0385 - accuracy: 0.9953 - val_loss: 2.9632 - val_accuracy: 0.4906\n",
            "Epoch 129/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0276 - accuracy: 0.9976 - val_loss: 2.9246 - val_accuracy: 0.5000\n",
            "Epoch 130/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.0251 - accuracy: 0.9976 - val_loss: 2.8847 - val_accuracy: 0.5000\n",
            "Epoch 131/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0261 - accuracy: 1.0000 - val_loss: 3.9118 - val_accuracy: 0.4340\n",
            "Epoch 132/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1499 - accuracy: 0.9479 - val_loss: 3.6171 - val_accuracy: 0.4906\n",
            "Epoch 133/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0337 - accuracy: 0.9976 - val_loss: 3.1144 - val_accuracy: 0.4811\n",
            "Epoch 134/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0307 - accuracy: 0.9976 - val_loss: 3.0670 - val_accuracy: 0.5000\n",
            "Epoch 135/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0217 - accuracy: 1.0000 - val_loss: 3.0346 - val_accuracy: 0.4906\n",
            "Epoch 136/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0266 - accuracy: 0.9953 - val_loss: 3.2410 - val_accuracy: 0.5189\n",
            "Epoch 137/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1134 - accuracy: 0.9668 - val_loss: 3.0089 - val_accuracy: 0.4811\n",
            "Epoch 138/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0236 - accuracy: 0.9976 - val_loss: 3.6967 - val_accuracy: 0.4528\n",
            "Epoch 139/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.1961 - accuracy: 0.9289 - val_loss: 3.0044 - val_accuracy: 0.5094\n",
            "Epoch 140/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0186 - accuracy: 1.0000 - val_loss: 2.8943 - val_accuracy: 0.5283\n",
            "Epoch 141/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0336 - accuracy: 0.9929 - val_loss: 3.0286 - val_accuracy: 0.5000\n",
            "Epoch 142/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0169 - accuracy: 1.0000 - val_loss: 3.0656 - val_accuracy: 0.4906\n",
            "Epoch 143/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0176 - accuracy: 1.0000 - val_loss: 3.1586 - val_accuracy: 0.5189\n",
            "Epoch 144/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0364 - accuracy: 0.9929 - val_loss: 3.0422 - val_accuracy: 0.4717\n",
            "Epoch 145/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1555 - accuracy: 0.9384 - val_loss: 3.1093 - val_accuracy: 0.5283\n",
            "Epoch 146/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0238 - accuracy: 0.9976 - val_loss: 3.0919 - val_accuracy: 0.4906\n",
            "Epoch 147/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0254 - accuracy: 0.9976 - val_loss: 3.9407 - val_accuracy: 0.4528\n",
            "Epoch 148/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1211 - accuracy: 0.9479 - val_loss: 2.9978 - val_accuracy: 0.5094\n",
            "Epoch 149/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0166 - accuracy: 1.0000 - val_loss: 3.1069 - val_accuracy: 0.4906\n",
            "Epoch 150/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 2.9793 - val_accuracy: 0.5000\n",
            "Epoch 151/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 3.3929 - val_accuracy: 0.5000\n",
            "Epoch 152/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0407 - accuracy: 0.9882 - val_loss: 3.5519 - val_accuracy: 0.4717\n",
            "Epoch 153/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0518 - accuracy: 0.9810 - val_loss: 3.4355 - val_accuracy: 0.5094\n",
            "Epoch 154/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0263 - accuracy: 0.9953 - val_loss: 3.2599 - val_accuracy: 0.4906\n",
            "Epoch 155/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 3.2797 - val_accuracy: 0.5000\n",
            "Epoch 156/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 3.1503 - val_accuracy: 0.5094\n",
            "Epoch 157/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 3.4397 - val_accuracy: 0.5283\n",
            "Epoch 158/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0720 - accuracy: 0.9739 - val_loss: 2.9945 - val_accuracy: 0.5660\n",
            "Epoch 159/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1058 - accuracy: 0.9668 - val_loss: 3.8665 - val_accuracy: 0.4340\n",
            "Epoch 160/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0145 - accuracy: 0.9976 - val_loss: 3.3593 - val_accuracy: 0.4811\n",
            "Epoch 161/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 3.2921 - val_accuracy: 0.4906\n",
            "Epoch 162/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 3.3441 - val_accuracy: 0.5000\n",
            "Epoch 163/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 3.4620 - val_accuracy: 0.4906\n",
            "Epoch 164/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 3.3870 - val_accuracy: 0.4906\n",
            "Epoch 165/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 3.4700 - val_accuracy: 0.4906\n",
            "Epoch 166/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 3.7904 - val_accuracy: 0.5000\n",
            "Epoch 167/200\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2144 - accuracy: 0.9194 - val_loss: 3.4222 - val_accuracy: 0.4717\n",
            "Epoch 168/200\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 3.7826 - val_accuracy: 0.4811\n",
            "Epoch 169/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 3.4984 - val_accuracy: 0.5000\n",
            "Epoch 170/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 3.7035 - val_accuracy: 0.4811\n",
            "Epoch 171/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 3.4359 - val_accuracy: 0.5000\n",
            "Epoch 172/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 3.4415 - val_accuracy: 0.5283\n",
            "Epoch 173/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0159 - accuracy: 0.9976 - val_loss: 3.9607 - val_accuracy: 0.4623\n",
            "Epoch 174/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 3.8515 - val_accuracy: 0.4811\n",
            "Epoch 175/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 4.0065 - val_accuracy: 0.4811\n",
            "Epoch 176/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0411 - accuracy: 0.9882 - val_loss: 4.4385 - val_accuracy: 0.4340\n",
            "Epoch 177/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0294 - accuracy: 0.9929 - val_loss: 3.7389 - val_accuracy: 0.4906\n",
            "Epoch 178/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 3.8192 - val_accuracy: 0.5094\n",
            "Epoch 179/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 3.8448 - val_accuracy: 0.4811\n",
            "Epoch 180/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 3.6977 - val_accuracy: 0.5000\n",
            "Epoch 181/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 3.8555 - val_accuracy: 0.4906\n",
            "Epoch 182/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 3.7813 - val_accuracy: 0.4906\n",
            "Epoch 183/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1201 - accuracy: 0.9668 - val_loss: 4.0782 - val_accuracy: 0.4811\n",
            "Epoch 184/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0389 - accuracy: 0.9882 - val_loss: 3.7197 - val_accuracy: 0.5000\n",
            "Epoch 185/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 3.7090 - val_accuracy: 0.4906\n",
            "Epoch 186/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 4.0190 - val_accuracy: 0.4717\n",
            "Epoch 187/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 3.9132 - val_accuracy: 0.4811\n",
            "Epoch 188/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 4.0312 - val_accuracy: 0.4906\n",
            "Epoch 189/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0849 - accuracy: 0.9692 - val_loss: 5.3049 - val_accuracy: 0.4245\n",
            "Epoch 190/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0404 - accuracy: 0.9929 - val_loss: 4.3387 - val_accuracy: 0.5000\n",
            "Epoch 191/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 3.9531 - val_accuracy: 0.5000\n",
            "Epoch 192/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 4.1291 - val_accuracy: 0.4906\n",
            "Epoch 193/200\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 4.0958 - val_accuracy: 0.4906\n",
            "Epoch 194/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 4.0998 - val_accuracy: 0.5000\n",
            "Epoch 195/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 4.0140 - val_accuracy: 0.5094\n",
            "Epoch 196/200\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 4.0744 - val_accuracy: 0.5000\n",
            "Epoch 197/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 4.4301 - val_accuracy: 0.4811\n",
            "Epoch 198/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2277 - accuracy: 0.9384 - val_loss: 4.1608 - val_accuracy: 0.4906\n",
            "Epoch 199/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0092 - accuracy: 0.9953 - val_loss: 3.8672 - val_accuracy: 0.5283\n",
            "Epoch 200/200\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 3.8664 - val_accuracy: 0.5189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUIjHHbjZMUR",
        "outputId": "00873b68-c99f-4422-8b76-530aa05ebda6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "model.evaluate(test,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15/15 [==============================] - 0s 2ms/step - loss: 1.9698 - accuracy: 0.7165\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.96980881690979, 0.7164502143859863]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pjph9oQeacLZ"
      },
      "source": [
        "**4th Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5CD8NsjZYpo",
        "outputId": "dbd1cf38-f2d9-430a-c9a1-0eda311c8d95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.datasets import fetch_covtype\n",
        "forest = fetch_covtype()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://ndownloader.figshare.com/files/5976039\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHiM0kObbBET",
        "outputId": "68a71a39-a9c3-4378-a2e4-a651f4c13328",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "forest=pd.Series(forest)\n",
        "forest"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "data      [[2596.0, 51.0, 3.0, 258.0, 0.0, 510.0, 221.0,...\n",
              "target    [5, 5, 2, 2, 5, 2, 5, 5, 5, 5, 5, 2, 2, 5, 5, ...\n",
              "DESCR     .. _covtype_dataset:\\n\\nForest covertypes\\n---...\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTlu8ojibEYz"
      },
      "source": [
        "forest_df=forest['data']\n",
        "forest_target=forest['target']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOxnjPPDbGKN",
        "outputId": "1745576f-dd1e-4257-f2fa-42384af9d71f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "forest_df=pd.DataFrame(forest_df)\n",
        "forest_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2596.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>258.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>510.0</td>\n",
              "      <td>221.0</td>\n",
              "      <td>232.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>6279.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2590.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>212.0</td>\n",
              "      <td>-6.0</td>\n",
              "      <td>390.0</td>\n",
              "      <td>220.0</td>\n",
              "      <td>235.0</td>\n",
              "      <td>151.0</td>\n",
              "      <td>6225.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2804.0</td>\n",
              "      <td>139.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>268.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>3180.0</td>\n",
              "      <td>234.0</td>\n",
              "      <td>238.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>6121.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2785.0</td>\n",
              "      <td>155.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>3090.0</td>\n",
              "      <td>238.0</td>\n",
              "      <td>238.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>6211.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2595.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>153.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>391.0</td>\n",
              "      <td>220.0</td>\n",
              "      <td>234.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>6172.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581007</th>\n",
              "      <td>2396.0</td>\n",
              "      <td>153.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>108.0</td>\n",
              "      <td>240.0</td>\n",
              "      <td>237.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>837.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581008</th>\n",
              "      <td>2391.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>240.0</td>\n",
              "      <td>237.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>845.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581009</th>\n",
              "      <td>2386.0</td>\n",
              "      <td>159.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>236.0</td>\n",
              "      <td>241.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>854.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581010</th>\n",
              "      <td>2384.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>230.0</td>\n",
              "      <td>245.0</td>\n",
              "      <td>143.0</td>\n",
              "      <td>864.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581011</th>\n",
              "      <td>2383.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>231.0</td>\n",
              "      <td>244.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>875.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>581012 rows Ã— 54 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0      1     2      3      4       5   ...   48   49   50   51   52   53\n",
              "0       2596.0   51.0   3.0  258.0    0.0   510.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "1       2590.0   56.0   2.0  212.0   -6.0   390.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "2       2804.0  139.0   9.0  268.0   65.0  3180.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "3       2785.0  155.0  18.0  242.0  118.0  3090.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "4       2595.0   45.0   2.0  153.0   -1.0   391.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "...        ...    ...   ...    ...    ...     ...  ...  ...  ...  ...  ...  ...  ...\n",
              "581007  2396.0  153.0  20.0   85.0   17.0   108.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "581008  2391.0  152.0  19.0   67.0   12.0    95.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "581009  2386.0  159.0  17.0   60.0    7.0    90.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "581010  2384.0  170.0  15.0   60.0    5.0    90.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "581011  2383.0  165.0  13.0   60.0    4.0    67.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "\n",
              "[581012 rows x 54 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTa3Ci13bJaG",
        "outputId": "6310c39b-f371-4578-eaf1-b3ee4bce1dbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 995
        }
      },
      "source": [
        "# Checking for NA values\n",
        "forest_df.isna().sum()*100/len(forest_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     0.0\n",
              "1     0.0\n",
              "2     0.0\n",
              "3     0.0\n",
              "4     0.0\n",
              "5     0.0\n",
              "6     0.0\n",
              "7     0.0\n",
              "8     0.0\n",
              "9     0.0\n",
              "10    0.0\n",
              "11    0.0\n",
              "12    0.0\n",
              "13    0.0\n",
              "14    0.0\n",
              "15    0.0\n",
              "16    0.0\n",
              "17    0.0\n",
              "18    0.0\n",
              "19    0.0\n",
              "20    0.0\n",
              "21    0.0\n",
              "22    0.0\n",
              "23    0.0\n",
              "24    0.0\n",
              "25    0.0\n",
              "26    0.0\n",
              "27    0.0\n",
              "28    0.0\n",
              "29    0.0\n",
              "30    0.0\n",
              "31    0.0\n",
              "32    0.0\n",
              "33    0.0\n",
              "34    0.0\n",
              "35    0.0\n",
              "36    0.0\n",
              "37    0.0\n",
              "38    0.0\n",
              "39    0.0\n",
              "40    0.0\n",
              "41    0.0\n",
              "42    0.0\n",
              "43    0.0\n",
              "44    0.0\n",
              "45    0.0\n",
              "46    0.0\n",
              "47    0.0\n",
              "48    0.0\n",
              "49    0.0\n",
              "50    0.0\n",
              "51    0.0\n",
              "52    0.0\n",
              "53    0.0\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0uyN7_TbLVc"
      },
      "source": [
        "forest_target=pd.DataFrame(forest_target)\n",
        "forest_df['Target']=forest_target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpVnH4hCbOKt"
      },
      "source": [
        "# Splitting the Data into Train and Test set\n",
        "train, validate, test = np.split(forest_df.sample(frac=1), [int(.6*len(forest_df)), int(.8*len(forest_df))])\n",
        "y_train=train['Target']\n",
        "y_val=validate['Target']\n",
        "y_test=test['Target']\n",
        "train.drop(columns={'Target'},axis='columns',inplace=True)\n",
        "validate.drop(columns={'Target'},axis='columns',inplace=True)\n",
        "test.drop(columns={'Target'},axis='columns',inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AjDLnDhbVwS"
      },
      "source": [
        "# Scaling the Data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "s=MinMaxScaler()\n",
        "train=pd.DataFrame(s.fit_transform(train))\n",
        "test=pd.DataFrame(s.fit_transform(test))\n",
        "validate=pd.DataFrame(s.fit_transform(validate))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jox_MAo_bXsk"
      },
      "source": [
        "# Preprocessing for Neural Networks\n",
        "from keras import Sequential\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Dense\n",
        "import tensorflow\n",
        "from keras.utils import to_categorical\n",
        "y_train=to_categorical(y_train)\n",
        "y_train=pd.DataFrame(y_train)\n",
        "y_val=to_categorical(y_val)\n",
        "y_test=to_categorical(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPNAvOFabZYZ",
        "outputId": "66bef17e-ad9d-456f-e211-2328a0215afb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        }
      },
      "source": [
        "# NN Model for Forest Dataset\n",
        "model=Sequential()\n",
        "model.add(Dense(units=104, activation='relu', input_dim=54))\n",
        "model.add(Dense(units=280, activation='relu'))\n",
        "model.add(Dense(units=380, activation='relu'))\n",
        "model.add(Dense(units=480, activation='relu'))\n",
        "model.add(Dense(units=580, activation='relu'))\n",
        "model.add(Dense(units=8, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "history=model.fit(train, y_train, epochs=20, verbose=1,batch_size=50,validation_data=(validate,y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "6973/6973 [==============================] - 20s 3ms/step - loss: 0.5689 - accuracy: 0.7548 - val_loss: 0.4593 - val_accuracy: 0.8074\n",
            "Epoch 2/20\n",
            "6973/6973 [==============================] - 20s 3ms/step - loss: 0.4105 - accuracy: 0.8258 - val_loss: 0.3723 - val_accuracy: 0.8414\n",
            "Epoch 3/20\n",
            "6973/6973 [==============================] - 19s 3ms/step - loss: 0.3422 - accuracy: 0.8557 - val_loss: 0.3260 - val_accuracy: 0.8630\n",
            "Epoch 4/20\n",
            "6973/6973 [==============================] - 20s 3ms/step - loss: 0.3024 - accuracy: 0.8726 - val_loss: 0.2912 - val_accuracy: 0.8794\n",
            "Epoch 5/20\n",
            "6973/6973 [==============================] - 20s 3ms/step - loss: 0.2741 - accuracy: 0.8849 - val_loss: 0.2826 - val_accuracy: 0.8837\n",
            "Epoch 6/20\n",
            "6973/6973 [==============================] - 19s 3ms/step - loss: 0.2552 - accuracy: 0.8930 - val_loss: 0.2448 - val_accuracy: 0.8998\n",
            "Epoch 7/20\n",
            "6973/6973 [==============================] - 20s 3ms/step - loss: 0.2399 - accuracy: 0.9001 - val_loss: 0.2479 - val_accuracy: 0.8987\n",
            "Epoch 8/20\n",
            "6973/6973 [==============================] - 19s 3ms/step - loss: 0.2278 - accuracy: 0.9049 - val_loss: 0.2244 - val_accuracy: 0.9083\n",
            "Epoch 9/20\n",
            "6973/6973 [==============================] - 19s 3ms/step - loss: 0.2178 - accuracy: 0.9098 - val_loss: 0.2377 - val_accuracy: 0.9020\n",
            "Epoch 10/20\n",
            "6973/6973 [==============================] - 19s 3ms/step - loss: 0.2087 - accuracy: 0.9130 - val_loss: 0.2186 - val_accuracy: 0.9115\n",
            "Epoch 11/20\n",
            "6973/6973 [==============================] - 19s 3ms/step - loss: 0.2026 - accuracy: 0.9167 - val_loss: 0.2111 - val_accuracy: 0.9143\n",
            "Epoch 12/20\n",
            "6973/6973 [==============================] - 19s 3ms/step - loss: 0.1975 - accuracy: 0.9178 - val_loss: 0.2135 - val_accuracy: 0.9133\n",
            "Epoch 13/20\n",
            "6973/6973 [==============================] - 19s 3ms/step - loss: 0.1904 - accuracy: 0.9214 - val_loss: 0.1968 - val_accuracy: 0.9207\n",
            "Epoch 14/20\n",
            "6973/6973 [==============================] - 19s 3ms/step - loss: 0.1884 - accuracy: 0.9227 - val_loss: 0.2015 - val_accuracy: 0.9172\n",
            "Epoch 15/20\n",
            "6973/6973 [==============================] - 19s 3ms/step - loss: 0.1814 - accuracy: 0.9253 - val_loss: 0.1977 - val_accuracy: 0.9206\n",
            "Epoch 16/20\n",
            "6973/6973 [==============================] - 20s 3ms/step - loss: 0.1810 - accuracy: 0.9260 - val_loss: 0.1910 - val_accuracy: 0.9239\n",
            "Epoch 17/20\n",
            "6973/6973 [==============================] - 19s 3ms/step - loss: 0.1844 - accuracy: 0.9271 - val_loss: 0.1860 - val_accuracy: 0.9252\n",
            "Epoch 18/20\n",
            "6973/6973 [==============================] - 19s 3ms/step - loss: 0.1706 - accuracy: 0.9300 - val_loss: 0.1947 - val_accuracy: 0.9221\n",
            "Epoch 19/20\n",
            "6973/6973 [==============================] - 19s 3ms/step - loss: 0.1703 - accuracy: 0.9302 - val_loss: 0.1858 - val_accuracy: 0.9270\n",
            "Epoch 20/20\n",
            "6973/6973 [==============================] - 20s 3ms/step - loss: 0.1679 - accuracy: 0.9316 - val_loss: 0.1942 - val_accuracy: 0.9258\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r91dZFLPbiGB",
        "outputId": "60b4888b-737e-4733-9a18-9a2d2747aac2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "model.evaluate(test,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3632/3632 [==============================] - 7s 2ms/step - loss: 0.1867 - accuracy: 0.9292\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.18670499324798584, 0.9291842579841614]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4g_LtgFUbivJ",
        "outputId": "8838425d-b587-4553-9151-88c8dc3a2ad5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2rLHME9bkoe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}